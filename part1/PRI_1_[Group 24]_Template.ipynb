{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implamentation is from group 24, composed by:\n",
    "\n",
    "- Vasco Morganho 81920\n",
    "- Diogo Moura 86976\n",
    "- Pedro Pereira 90766\n",
    "\n",
    "\n",
    "This Notebook showcases the functional part of the first delivery. In each section we  present the function and a set of outputs. After each funtion we will mention the structure and the meaning of each input and output. Alternatively, you can include a standard funtion signature.\n",
    "\n",
    "Note: This notebook is not functional. Its main goal is to provide instructions on how to call the functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a) \n",
    "\n",
    "def indexing(D,args = {'topicsFile': 'topics.txt', 'relevanceFile': 'qrels.test', 'preprocessing': False, 'dictFile': 'no-preprocessed-dictionary.comp'}):\n",
    "    parsingTime = 0\n",
    "    indexingTime = 0\n",
    "    memoryAtStart = process.memory_info().rss\n",
    "    if (args.get('dictFile') != None and args.get('dictFile') in listdir('.')):\n",
    "        pickleFile = open(args.get('dictFile'), 'rb')\n",
    "        iIndex = pickle.load(pickleFile)\n",
    "        pickleFile.close()\n",
    "        return (iIndex, (0, round(iIndex.getSize()/(1024*1024),2)))\n",
    "\n",
    "    if(args.get('topicsFile') == None or args.get('relevanceFile') == None or args.get('preprocessing') == None):\n",
    "        raise ValueError(\"Missing arguments!\\nArguments dictionary must contain 'topicFile', 'relevanceFile' and 'tokenizationType' fields!\")\n",
    "\n",
    "    evaledDocs = getEvaledDocs(args.get('relevanceFile')) \n",
    "    filteredD = list(set(filter(lambda x: re.search('/[^/]*?newsML\\.xml', x).group()[1:] in evaledDocs ,D)))\n",
    "\n",
    "    start = time.time()\n",
    "    iIndex = DocumentInvertedIndex('Train',args.get('topicsFile'), args.get('relevanceFile'), args.get('preprocessing'))\n",
    "    \n",
    "\n",
    "    sizeD = len(filteredD)\n",
    "    for i in tqdm(range(0, sizeD)):\n",
    "        times = iIndex.addToIndex(filteredD[i])\n",
    "        parsingTime += times[0]\n",
    "        indexingTime += times[1]\n",
    "    \n",
    "        \n",
    "    iIndex.calcAllIdfs()\n",
    "    end = time.time()\n",
    "    iIndex.calcSize()\n",
    "    size=round(iIndex.getSize()/(1024*1024),2)#size rounded to 2 decimal places\n",
    "    totalTime=round(end-start,3)\n",
    "\n",
    "    if(args.get('dictFile') != None):\n",
    "        with open(args.get('dictFile'), 'wb') as dictFile:\n",
    "            pickle.dump(iIndex, dictFile)\n",
    "    memoryAtEnd = process.memory_info().rss\n",
    "    print(\"Parsing Time: {0}s\".format(round(parsingTime, 2)))\n",
    "    print(\"Indexing Time: {0}s\".format(round(indexingTime, 2)))\n",
    "\n",
    "    return (iIndex, (totalTime, (memoryAtEnd - memoryAtStart) / (1024*1024) ))#returns tuple with (iIndex,(time(s),sizeOccupied(MB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__@input:__\n",
    "\n",
    "__D__ list of strings that corresponds to the path of documents to be indexed.\n",
    "\n",
    "__args:__ a dictionary with the following keys: topicsFile (file path that contains the topics), \n",
    "          relevanceFile (file that contains the relevance feedback), preprocessing (boolean), \n",
    "          dictFile (file in binary form from which an inverted index can be retrieved. If the file does not exist\n",
    "          the inverted index will be stored in the provided file. This argument is optional)\n",
    "\n",
    "__@output__\n",
    "\n",
    "A tuple with the folwoing structure (an instance of DocumentInvertedIndex, (total time of parsing, processing and parsing, size of the inverted index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b)\n",
    "\n",
    "def extract_topic_query(q, I, k,args = {'metric': 'TF-IDF'}):\n",
    "    topic = None\n",
    "    q = q.lower()\n",
    "\n",
    "    if(args.get('metric') == None):\n",
    "        raise ValueError(\"You must choose a metric! 'TF-IDF' or 'TF'\")\n",
    "\n",
    "    try:\n",
    "        topic = I.topicIndex.dictionary[q]\n",
    "    except KeyError:\n",
    "        print(\"Query Topic Not Found\")\n",
    "        return None\n",
    "\n",
    "    topic = topic[2:]\n",
    "    \n",
    "    topicTerms = list(map(lambda x: x[0], topic))\n",
    "    dictTerms = list(filter(lambda x: x in topicTerms, I.getTerms()))\n",
    "\n",
    "    if(args.get('metric') == 'TF'):\n",
    "        # Sum of term frequencies TF \n",
    "        paramterizedTerms = [(t, I.getPosting(t)[0]) for t in dictTerms]\n",
    "        paramterizedTerms = sorted(paramterizedTerms, reverse = True, key = (lambda x: x[1]))[0:k]\n",
    "\n",
    "    elif(args.get('metric') == 'TF-IDF'):\n",
    "        #TF*IFDF   TF is in I.getPosting(t)[0] and IDF is in I.getPosting(t)[1] \n",
    "        paramterizedTerms = [(t, I.getPosting(t)[0]*I.getPosting(t)[1]) for t in dictTerms]\n",
    "        paramterizedTerms = sorted(paramterizedTerms, reverse = True, key = (lambda x: x[1]))[0:k]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric '{0}' ! Valid Options: 'TF-IDF' or 'TF'\".format(args.get('metric')))\n",
    "    \n",
    "    return list(map(lambda x: x[0], paramterizedTerms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__@input:__\n",
    "\n",
    "__q__ is a string (the topic num, e.g. 'R101')\n",
    "\n",
    "__k__ is a integer (the size of the extracted query)\n",
    "\n",
    "__args__: a dictionary with a key: metric(string corresponding to the metric to extract the query from. 'TF-IDF' or 'TF')\n",
    "\n",
    "__@output__\n",
    "\n",
    "list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(c)\n",
    "\n",
    "def boolean_query(q,I,k,args = {'metric': 'TF-IDF'}):\n",
    "    query=extract_topic_query(q,I,k, args)\n",
    "    documents=dict()\n",
    "    for term in query:\n",
    "        for doc_id,tf in I.dictionary[term][2:]:#term[0] is the term and term[1] is the tf\n",
    "            if doc_id not in documents:\n",
    "                documents[doc_id]=1\n",
    "            else:\n",
    "                documents[doc_id]+=1\n",
    "\n",
    "    documents = [(k, v) for k,v in documents.items()]\n",
    "    min_term_frequency = math.floor(0.8*len(query))\n",
    "    documents = sorted(documents, key = lambda x : x[1], reverse = True)\n",
    "    documents=[doc_id for doc_id,tf in documents if tf >= min_term_frequency]\n",
    "    return list(filter(lambda x:x in I.topicIndex.getRelevantDocsForTopic(q.lower()) \n",
    "                       or x in I.topicIndex.getNonRelevantDocsForTopic(q.lower()),documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__@input:__\n",
    "\n",
    "__q__ is a string (the topic num, e.g. 'R101')\n",
    "\n",
    "__k__ is a integer\n",
    "\n",
    "__args__: a dictionary with a key: metric(string corresponding to the metric to extract the query from. Possible values 'TF-IDF' or 'TF'. This is meant to be passed to the extract_topic_query)\n",
    "\n",
    "__@output__\n",
    "\n",
    "filtered collection is a ordered list of document identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(d)\n",
    "\n",
    "def ranking(q,p,I,args = {'metric': 'TF-IDF'}):\n",
    "    if(args.get('metric') == 'TF-IDF'):\n",
    "        similarities = rankingTFIDF(q, p, I)\n",
    "    elif(args.get('metric') == 'BM25'):\n",
    "        similarities = rankingBM25(q, p, I)\n",
    "    elif(args.get('metric') == 'RRF'):\n",
    "        similarities1 = rankingTFIDF(q, p, I)\n",
    "        similarities2 = rankingBM25(q, p, I)\n",
    "        similarities = RRF([similarities1, similarities2])\n",
    "    else:\n",
    "        raise ValueError(\"You must choose a ranking function: 'TF-IDF' or 'BM25' or 'RRF or 'cosSim'\")\n",
    "    similarities = [(k, v) for k,v in similarities.items()]\n",
    "    similarities = [s for s in similarities if (s[0] in I.topicIndex.getRelevantDocsForTopic(q) or s[0] in I.topicIndex.getNonRelevantDocsForTopic(q))]   \n",
    "    similarities = sorted(similarities, key= lambda x: x[1], reverse=True)\n",
    "    similarities=list(filter(lambda x:x[0] in I.topicIndex.getRelevantDocsForTopic(q.lower()) \n",
    "                       or x[0] in I.topicIndex.getNonRelevantDocsForTopic(q.lower()),similarities))\n",
    "    return similarities[0:p]\n",
    "    \n",
    "def rankingTFIDF(q,p,I):\n",
    "    similarities = {}\n",
    "    for term in I.topicIndex.dictionary[q.lower()]:#iterate over all the terms in the doc query\n",
    "        tfidf_topic = term[1] #get the topic tf-idf from the dictionary of the topic\n",
    "        try:\n",
    "            iIndex=I.dictionary[term[0]] #get the postings from the term in the document Inverted Index\n",
    "        except KeyError:\n",
    "            continue\n",
    "        idf=iIndex[1] #get the idf from the term posting (in the docment Inverted Index)\n",
    "        for pair in iIndex[2:]:\n",
    "            docId = pair[0]\n",
    "            TF_df = pair[1]\n",
    "            if docId not in similarities.keys():\n",
    "                similarities[docId] = 0\n",
    "            similarities[docId] += math.log(1+TF_df,10)* idf * math.log(1+tfidf_topic,10) * idf\n",
    "    return similarities\n",
    "\n",
    "def rankingBM25(q, p, I):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    avgDocSum = 0\n",
    "    for i in I.document_lengths.keys():\n",
    "        avgDocSum += I.document_lengths[i]\n",
    "    avgDocLen =  avgDocSum / len(I.document_lengths.keys())\n",
    "    similarities = { } #idf, #doc_tf\n",
    "    for term in I.topicIndex.dictionary[q.lower()]:\n",
    "        postings = I.getPosting(term[0])\n",
    "        if (postings == None):\n",
    "            continue\n",
    "        idfTerm = postings[1]\n",
    "        for posting in postings[2:]:\n",
    "            docId = posting[0]\n",
    "            tf = posting[1]\n",
    "\n",
    "            if (similarities.get(docId) == None):\n",
    "                similarities[docId] = 0\n",
    "            similarities[docId] += idfTerm * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (I.document_lengths[docId]/ avgDocLen))))\n",
    "    return similarities\n",
    "\n",
    "def RRF(rankingsList):\n",
    "    rrf = { }\n",
    "    for doc in rankingsList[0].keys():\n",
    "        if (rrf.get(doc) == None):\n",
    "            rrf[doc] = 0\n",
    "        for ranking in rankingsList:\n",
    "            rrf[doc] += (1 / 50 + ranking[doc])\n",
    "    return rrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__@input:__\n",
    "\n",
    "__q__ is a string\n",
    "\n",
    "__p__ is a integer\n",
    "\n",
    "__args__: a dictionary with a key: metric(string corresponding to the scoring metric.Possible Values: 'TF-IDF' ,'BM25' or 'RRF')\n",
    "\n",
    "__@output__\n",
    "\n",
    "ordered set of top-p documents is a list of pairs (document identifier, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(e) \n",
    "\n",
    "def evaluation(Q_test,R_test,D_test,args = {'k':10, 'rankingMeasure': 'TF-IDF','booleanMetric': 'TF-IDF', 'queryType': 'ranked'}):\n",
    "    iIndex=D_test\n",
    "    queryStartTime = 0\n",
    "    queryEndTime = 0\n",
    "    timeSum = 0\n",
    "    queriesDone = 0\n",
    "    statsObject = {}\n",
    "    k=args.get('k')\n",
    "    if k == None or (not isinstance(k,int)):\n",
    "        raise ValueError(\"Invalid k '{0}' !\".format(args.get('k')))\n",
    "    for topic in Q_test:\n",
    "        statsObject[topic] = {}\n",
    "        query_result=None\n",
    "        if (args.get('queryType') != None and args.get('queryType') == 'boolean'):\n",
    "                queryStartTime = time.time()\n",
    "                query_result=boolean_query(topic,iIndex,k, {'metric': args.get('booleanMetric')})\n",
    "                queryEndTime = time.time()\n",
    "                queriesDone += 1\n",
    "\n",
    "        elif (args.get('queryType') != None and args.get('queryType') == 'ranked'):\n",
    "                queryStartTime = time.time()\n",
    "                query_result = list(map(lambda x: x[0], ranking(topic,k,iIndex, {'metric': args.get('rankingMeasure')})))\n",
    "                queryEndTime = time.time()\n",
    "                queriesDone += 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid query type '{0}' !\".format(args.get('queryType')))\n",
    "\n",
    "        timeSum += queryEndTime - queryStartTime\n",
    "\n",
    "        if (query_result != None):\n",
    "            relevantFiles=R_test.getRelevantDocsForTopic(topic)\n",
    "            nonRelevantFiles = R_test.getNonRelevantDocsForTopic(topic)\n",
    "            \n",
    "            recallValues,precisionAtRecall=precisionRecallCurve(query_result, relevantFiles,nonRelevantFiles)\n",
    "            statsObject[topic]['precision-at-recall'] = precisionAtRecall\n",
    "\n",
    "            statsObject[topic]['MAP'] = MAP(query_result, relevantFiles, nonRelevantFiles)\n",
    "            statsObject[topic]['BPREF'] = BPREF(query_result, relevantFiles, nonRelevantFiles)\n",
    "            statsObject[topic]['Precision'] = calcPrecision(query_result, relevantFiles, nonRelevantFiles)\n",
    "            statsObject[topic]['Recall'] = calcRecall(query_result, relevantFiles, nonRelevantFiles)\n",
    "            statsObject[topic]['efficiency'] = queryEndTime - queryStartTime\n",
    "    return statsObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__@input:__\n",
    "\n",
    "__Qtest__ is alist of string containing topic numbers.\n",
    "\n",
    "__Rtest__ is an instance of TopicIndex (can be accessed through DocumentInvertedIndex.topicIndex)\n",
    "\n",
    "__Dtest__ is an instance of DocumentInvertedIndex (returned by the indexing function) \n",
    "\n",
    "__args__: a dictionary with the following keys/ possible values:\n",
    "            'k' an integer to pass to the bolean_query as k or to pass to ranking as p;\n",
    "            'rankingMeasure': 'TF-IDF', 'BM25', 'RRF'. Passed to the ranking function;\n",
    "            'queryType': 'ranked' or 'boolean' \n",
    "\n",
    "__@output__ a dictionary of dictionaries. There's an entry in the first one for each topic in Qtest. Each of these\n",
    "            entries contains a dictionary with the following entries corresponding to measures: \n",
    "            'precision-at-recall', 'MAP', 'BREF', 'Precision' and 'Recall'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further functionalities implemented if thats the case\n",
    "\n",
    "#text processing options (e.g. absence versus presence of phrase extraction); \n",
    "# IR models (including the Boolean, TF-IDF and BM25 models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__@input:__ and __@output__ of each funtion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
