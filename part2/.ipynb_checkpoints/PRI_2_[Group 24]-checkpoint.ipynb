{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our collection is in the same directory as the code files. The folder is called rcv1, which has 2 folders inside: D_train and D_test. Each one of those contains folders corresponding to a certain day and each one of those folders contains documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4557/4557 [01:27<00:00, 52.37it/s]\n",
      "100%|██████████| 33920/33920 [10:28<00:00, 53.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "D_train = getEvaledDocs('qrels.train')\n",
    "D_test = getEvaledDocs('qrels.test')\n",
    "trainIndex = WooshDocumentIndex(False, \"trainIndex\", D_train)\n",
    "testIndex = WooshDocumentIndex(False, \"testIndex\", D_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndex=TopicIndex(\"topics.txt\",\"topicsindexdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from Preprocess import Preprocess\n",
    "from xml_documents_parser import DocumentParser\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from scipy.sparse import * \n",
    "from util import *\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import functools\n",
    "import sys\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "def file_to_string(file):\n",
    "    ptr=open(file)\n",
    "    _str=ptr.read()        \n",
    "    return _str \n",
    "\n",
    "def calcOptimNclusters(vectorspace, krange=(2,30)):\n",
    "    model = KMeans()\n",
    "    visualizer = KElbowVisualizer(model, k=krange, metric='silhouette', timings= True)\n",
    "    visualizer.fit(vectorspace)        # Fit the data to the visualizer\n",
    "    visualizer.show()\n",
    "    exit()\n",
    "    \n",
    "def cohesion(labels, distances):\n",
    "    sse = 0\n",
    "    sseDict = {}\n",
    "    uniqueLabels = np.unique(labels)\n",
    "    for label in uniqueLabels:\n",
    "        sse = 0\n",
    "        clusterDocs = np.where(labels == label)[0]\n",
    "        for doc1 in clusterDocs:\n",
    "            for doc2 in clusterDocs:\n",
    "                sse += (pointDistance(distances[doc1], distances[doc2])/2) ** 2\n",
    "        sseDict[label] = sse\n",
    "    return sseDict\n",
    "\n",
    "def separation(labels, distances):\n",
    "    ssb = 0\n",
    "    ssbDict = {}\n",
    "    uniqueLabels = np.unique(labels)\n",
    "    for label in uniqueLabels:\n",
    "        ssb = 0\n",
    "        clusterDocs = np.where(labels == label)[0]\n",
    "        for doc1 in clusterDocs:\n",
    "            nonClsuterDocs = np.where(labels != label)[0]\n",
    "            for doc2 in nonClsuterDocs:\n",
    "                ssb += (pointDistance(distances[doc1], distances[doc2])/2) ** 2\n",
    "        ssbDict[label] = ssb\n",
    "    return ssbDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "__@input:__\n",
    "\n",
    "__D__ has the structure of a list of filenames or a list of topic names\n",
    "\n",
    "__col__ 'docs' for documents, anything else for topics\n",
    "\n",
    "__@output__\n",
    "\n",
    "__clustersDescription__ list of pairs medoid coordinates and list of documents\n",
    "\n",
    "__clusters__ array of arrays of document id's\n",
    "\n",
    "__vectorspace__ Tfidfvectorizer's fit transform output\n",
    "\n",
    "__labels__ labels\n",
    "\n",
    "__distances__ distances vector of distances between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(D, col, number_clusters, minDocFreq = 2, maxDocFreq = 0.9):\n",
    "    if(col == 'docs'):\n",
    "        D = noPreprocessDocs(D)\n",
    " \n",
    "    vectorizer = TfidfVectorizer(use_idf = False, min_df= minDocFreq, max_df=maxDocFreq, stop_words='english')\n",
    "    vectorspace = vectorizer.fit_transform(D)\n",
    "    \n",
    "\n",
    "    kmeans = KMeans(n_clusters=number_clusters)\n",
    "    labels = kmeans.fit_predict(vectorspace)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    distances = kmeans.fit_transform(vectorspace.toarray())\n",
    "\n",
    "    pca = PCA(2)\n",
    "    distances = pca.fit_transform(distances)\n",
    "\n",
    "    uniqueLabels = np.unique(labels)\n",
    "    clustersDescription = []\n",
    "    clusters = []\n",
    "    for i in uniqueLabels:\n",
    "        clustersDescription.append(((centroids[:,0][i], centroids[:,0][i]), list(map(lambda x: D[x], list(np.where(labels == i)[0])))))\n",
    "        clusters.append(np.where(labels == i)[0])\n",
    "    return clustersDescription, clusters, vectorspace, labels, distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret\n",
    "__@input:__\n",
    "\n",
    "__cluster__ array of document id's\n",
    "\n",
    "__D__ has the structure of a list of filenames or a list of topic name\n",
    "\n",
    "__col__ 'docs' for document analysis, 'topics' for topic analysis\n",
    "\n",
    "__@output__\n",
    "\n",
    "__medoid__ document Id or topic name\n",
    "\n",
    "__mean__ float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret(cluster, D, distances, col):\n",
    "    nDistances = len(distances)\n",
    "    distanceMatrix = {}\n",
    "\n",
    "    for i in  range(nDistances):\n",
    "        if i not in cluster: \n",
    "            continue\n",
    "        pointDistances = []\n",
    "        for j in range(nDistances):\n",
    "            if j not in cluster: \n",
    "                continue\n",
    "            pointDistances.append(pointDistance(distances[i], distances[j]))\n",
    "        distanceMatrix[i] = pointDistances\n",
    "    min = sys.maxsize\n",
    "    medoid = 0\n",
    "    for i in distanceMatrix.keys():\n",
    "        j = float(functools.reduce(lambda x, y: x + y, distanceMatrix[i]))\n",
    "        if j < min:\n",
    "            min = j\n",
    "            medoid = i\n",
    "    mean = np.mean(list(distanceMatrix.values()))\n",
    "    if(col == 'docs'):\n",
    "        medoid = D[medoid]\n",
    "    elif(col == 'topics'):\n",
    "        medoid = 'R' + str(101+medoid)\n",
    "    return medoid, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "__@input:__\n",
    "\n",
    "__D__ has the structure of a list of filenames or a list of topic name\n",
    "\n",
    "__vectorspace__ Tfidfvectorizer's fit transform output\n",
    "\n",
    "__labels__ labels\n",
    "\n",
    "__distances__ distances vector of distances between documents\n",
    "\n",
    "__col__ 'docs' for document analysis, 'topics' for topic analysis\n",
    "\n",
    "__@output__\n",
    "\n",
    "__sil_score__ float\n",
    "\n",
    "__avgCohesion__ float\n",
    "\n",
    "__avgSeparation__ float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluation(D, vectorspace, labels, distances):\n",
    "    sil_score = silhouette_score(vectorspace, labels, metric = 'cosine')\n",
    "    #print('silhouette score = {}'.format(sil_score))\n",
    "    avgCoheision = np.average(list(cohesion(labels, distances).values()))\n",
    "    avgSeparation = np.average(list(separation(labels, distances).values()))\n",
    "    return sil_score, avgCoheision, avgSeparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'interpret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-712d22a4c808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'docs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmedoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'docs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Medoid: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpret' is not defined"
     ]
    }
   ],
   "source": [
    "D = []\n",
    "for i in ['R101' , 'R121', 'R150', 'R170', 'R180']:\n",
    "    D += getEvaledDocsForTopic('qrels.test', i, 'test')\n",
    "description, clusters, vectorspace, labels, distances = clustering(D, 'docs', number_clusters = 11)\n",
    "print(description)\n",
    "medoid, mean = interpret(clusters[0], D, distances, 'docs')\n",
    "print(\"Medoid: \", medoid)\n",
    "print(\"Mean: \", mean)\n",
    "sil_score, avgCoheision, avgSeparation = evaluation(D, vectorspace, labels, distances)\n",
    "print(\"Silhouette: \", sil_score)\n",
    "print(\"AvgCoheision: \", avgCoheision)\n",
    "print(\"AvgSeparation: \", avgSeparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "__@input:__\n",
    "\n",
    "__q__ string with topic name\n",
    "\n",
    "__Dtrain__ list of document names\n",
    "\n",
    "__Rtrain__ relevance feedback list\n",
    "\n",
    "__distances__ distances vector of distances between documents\n",
    "\n",
    "__calssifier_type__ 'logistic' or 'XGBOOST'\n",
    "\n",
    "__@output__\n",
    "\n",
    "__classifier__ classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic import LogisticClassifier\n",
    "from xGBoost import XGBOOSTClassifier\n",
    "from util import getCollection,setupEvalManyFeatures,setupEvalOneFeature\n",
    "from util import getRelevantNonRelevant,compute_metrics,RRF\n",
    "from util import show_graphics_metrics_IRmodels,show_metrics_classifiers\n",
    "from statistics import mean\n",
    "from woosh_index import WooshDocumentIndex,TopicIndex\n",
    "\n",
    "def training(q,DTrain,RTrain,**args):\n",
    "    '''\n",
    "    @input topic document q ∈ Q, training collection Dtrain, judgments Rtrain, and\n",
    "    optional arguments on the classification process\n",
    "    @behavior learns a classification model to predict the relevance of documents on the\n",
    "    topic q using Dtrain and Rtrain, where the training process is subjected to\n",
    "    proper preprocessing, classifier’s selection and hyperparameterization\n",
    "    @output q-conditional classification model\n",
    "    '''\n",
    "    classifier_type = args.get('classifier_type')\n",
    "    if classifier_type=='logistic':\n",
    "        classifier=LogisticClassifier()\n",
    "    elif classifier_type=='XGBOOST':\n",
    "        classifier=XGBOOSTClassifier()\n",
    "    classifier.train(q,DTrain,RTrain)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify\n",
    "\n",
    "__@input:__\n",
    "\n",
    "__d__ document features (document belongs to Dtest)\n",
    "\n",
    "__q__ topic name\n",
    "\n",
    "__M__ classifier model\n",
    "\n",
    "\n",
    "__@output__\n",
    "\n",
    "__probability__ proabability of document d being classified as relvant for topic q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(d,q,M,**args):\n",
    "    '''\n",
    "    @input document d ∈ Dtest, topic q ∈ Q, and classification model M\n",
    "    @behavior classifies the probability of document d to be relevant for topic q given M\n",
    "    @output probabilistic classification output on the relevance of document d to the\n",
    "    topic t\n",
    "    '''\n",
    "    classifier=M[q]\n",
    "    return classifier.classify(d,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "__@input:__\n",
    "\n",
    "__Qtest__ list of topic names\n",
    "\n",
    "__Dtest__ list of document names\n",
    "\n",
    "__RTrain__ list of relevance feedback for train collection\n",
    "\n",
    "__RTest__ list of relevance feedback for test collection\n",
    "\n",
    "__trainX__ list of document features for train collection\n",
    "\n",
    "__testX__ list of document features for test collection\n",
    "\n",
    "__@output__\n",
    "\n",
    "__aidedRanked__ statistics for ranked retrieval aided by relevance relevance feedback \n",
    "\n",
    "__aidedNonRanked__ statistics for simple retrieval aided by relevance relevance feedback \n",
    "\n",
    "__nonAided__ statistics for simple retrieval\n",
    "\n",
    "__classifier_metrics__ general statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Qtest,DTest,**args):\n",
    "    '''\n",
    "    @input subset of topics Qtest ⊆ Q, testing document collection Dtest, judgments\n",
    "    Rtest, and arguments for the classification and retrieval modules\n",
    "    @behavior evaluates the behavior of the IR system in the presence and absence of\n",
    "    relevance feedback. In the presence of relevance feedback, training and\n",
    "    testing functions are called for each topic in Qtest for a more comprehensive assessment\n",
    "    @output performance statistics regarding the underlying classification system and\n",
    "    the behavior of the aided IR system\n",
    "    '''\n",
    "    RTrain=args.get('RTrain')\n",
    "    RTest=args.get('RTest')\n",
    "    trainX=args.get('trainX')\n",
    "    testX=args.get('testX')\n",
    "    classifier_type=args.get('classifier_type')\n",
    "    Model=dict()\n",
    "    #metrics\n",
    "    aidedRanked=dict()\n",
    "    aidedNonRanked=dict()\n",
    "    nonAided = dict()\n",
    "    classifier_metrics=dict()\n",
    "    #k used for retrieval\n",
    "    k=args.get('k')\n",
    "    ranking_type=args.get('ranking_type')\n",
    "    for topic in Qtest:\n",
    "        relevant,nonRelevant=getRelevantNonRelevant(topic)\n",
    "        '''\n",
    "        Non-Aided IR\n",
    "        '''\n",
    "        ranked_docs_names=[name for score, name in sorted(zip(testX[topic],DTest[topic]), \n",
    "                            key=lambda pair: RRF(pair[0]),reverse=True)]\n",
    "        precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec=compute_metrics(ranked_docs_names\n",
    "                                                                          ,relevant, nonRelevant,k)\n",
    "        nonAided[topic] = [precision, recall, fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "        '''\n",
    "        Train the model\n",
    "        '''\n",
    "        try:\n",
    "            Model[topic]=training(topic,trainX,RTrain,classifier_type=classifier_type)\n",
    "            aidedNonRanked[topic],aidedRanked[topic],classifier_metrics[topic]=Model[topic].evaluate(topic,\n",
    "                          DTest,RTest,k = k,testX=testX,ranking_type=ranking_type,\n",
    "                          relevant=relevant,nonRelevant=nonRelevant)\n",
    "        except ValueError:\n",
    "            print(\"For topic \", topic\n",
    "                  ,\"the classifier needs samples of at least 2 classes in the data\"\n",
    "                  , \"but the data contains only one class: 1\")\n",
    "            aidedNonRanked[topic]=nonAided[topic]\n",
    "            aidedRanked[topic]=nonAided[topic]\n",
    "            #values from Non Aided\n",
    "            classifier_metrics[topic]=[precision,recall,fscoreVal,avg_prec]\n",
    "        \n",
    "    '''\n",
    "    Calculate Average values for the metrics\n",
    "    '''\n",
    "    nonAided['Avg MAP'] = mean([nonAided[topic][5] for topic in nonAided])\n",
    "    nonAided['Avg BPREF'] = mean([nonAided[topic][4] for topic in nonAided if topic != 'Avg MAP'])\n",
    "    \n",
    "    aidedRanked['Avg MAP']= mean([aidedRanked[topic][5] for topic in aidedRanked])\n",
    "    aidedRanked['Avg BPREF']= mean([aidedRanked[topic][4] for topic in aidedRanked if topic != 'Avg MAP'])\n",
    "    \n",
    "    aidedNonRanked['Avg MAP']= mean([aidedNonRanked[topic][5] for topic in aidedNonRanked])\n",
    "    aidedNonRanked['Avg BPREF']= mean([aidedNonRanked[topic][4] for topic in aidedNonRanked if topic != 'Avg MAP'])\n",
    "\n",
    "    classifier_metrics['Avg MAP']=mean([classifier_metrics[topic][3] for topic in classifier_metrics])    \n",
    "    return aidedRanked,aidedNonRanked,nonAided,classifier_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtest=['R'+str(i) for i in range(101,201,1)]\n",
    "#Qtest=['R103','R102','R142','R145','R131','R145','R198']#,'R145']#,'R106','R107','R195','R175']\n",
    "\n",
    "trainfiles,testfiles,DTrain,RTrain,DTest,RTest=getCollection(Qtest)\n",
    "trainDocsIndex=WooshDocumentIndex(load=True,dir_name=\"trainIndex\",files=trainfiles)\n",
    "testDocsIndex=WooshDocumentIndex(load=True,dir_name=\"testIndex\",files=testfiles)\n",
    "topicIndex=TopicIndex(\"topics.txt\",\"topicsindexdir\")\n",
    "\n",
    "features='many'#features='one' for only one feature in the vector (bm25)\n",
    "if features=='many':\n",
    "    trainX, testX = setupEvalManyFeatures(Qtest, DTrain, DTest,\n",
    "                              trainDocsIndex,testDocsIndex,topicIndex)\n",
    "elif features=='one':\n",
    "    trainX, testX = setupEvalOneFeature(Qtest, DTrain, DTest,\n",
    "                                  trainDocsIndex,testDocsIndex,topicIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierModel=dict()\n",
    "classifierModel['R103']=training('R103',trainX,RTrain,classifier_type='logistic')\n",
    "classifierModel['R102']=training('R102',trainX,RTrain,classifier_type='logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT WITH ID  2738  with probabilities  [0.77665883 0.22334117] for topic R103\n",
      "DOCUMENT WITH ID  6062  with probabilities  [0.71780783 0.28219217] for topic R103\n"
     ]
    }
   ],
   "source": [
    "document1_name=DTrain['R103'][0]#choose first document in the collection\n",
    "document1_features=trainX['R103'][0]#pick the feature representation\n",
    "probabilities1=classify(document1_features,'R103',classifierModel)\n",
    "print('DOCUMENT WITH ID ',document1_name, ' with probabilities ', probabilities1,'for topic R103')\n",
    "document2_name=DTrain['R103'][1]\n",
    "document2_features=trainX['R103'][1]\n",
    "probabilities2=classify(document2_features,'R103',classifierModel)\n",
    "print('DOCUMENT WITH ID ',document2_name, ' with probabilities ', probabilities2,'for topic R103')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For topic  R175 the classifier needs samples of at least 2 classes in the data but the data contains only one class: 1\n",
      "Best Score: 0.74\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.773974358974359\n",
      "Best Hyperparameters: {'C': 0.04716237184469433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7820512820512822\n",
      "Best Hyperparameters: {'C': 0.0002012896661878088, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7878542510121458\n",
      "Best Hyperparameters: {'C': 0.013653836035069424, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7785714285714287\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9333333333333332\n",
      "Best Hyperparameters: {'C': 0.015864765456573147, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9512820512820512\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9454545454545453\n",
      "Best Hyperparameters: {'C': 0.0002012896661878088, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9450292397660819\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9436363636363637\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9651515151515152\n",
      "Best Hyperparameters: {'C': 33.72108309441093, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8824175824175823\n",
      "Best Hyperparameters: {'C': 0.0043963310508178515, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9199999999999999\n",
      "Best Hyperparameters: {'C': 0.015864765456573147, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9355555555555555\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.6733333333333332\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9666666666666666\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8466666666666667\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8345454545454546\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8639705882352942\n",
      "Best Hyperparameters: {'C': 0.0002012896661878088, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8428571428571429\n",
      "Best Hyperparameters: {'C': 0.0004493894149083743, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9618181818181817\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.819047619047619\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7535714285714287\n",
      "Best Hyperparameters: {'C': 0.0005290265559831401, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.86\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8428571428571429\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9218181818181819\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.819047619047619\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.96\n",
      "Best Hyperparameters: {'C': 33.72108309441093, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9047619047619048\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9419047619047619\n",
      "Best Hyperparameters: {'C': 0.01970088126798927, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9155555555555555\n",
      "Best Hyperparameters: {'C': 0.015864765456573147, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8380952380952381\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7266666666666668\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8688888888888888\n",
      "Best Hyperparameters: {'C': 0.007722748477617412, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9800000000000001\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9489473684210525\n",
      "Best Hyperparameters: {'C': 0.0002012896661878088, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9099999999999999\n",
      "Best Hyperparameters: {'C': 0.015864765456573147, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8803030303030303\n",
      "Best Hyperparameters: {'C': 0.015864765456573147, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8075757575757576\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.86\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9236363636363636\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9200000000000002\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9473684210526315\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8142857142857143\n",
      "Best Hyperparameters: {'C': 0.007722748477617412, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9358974358974359\n",
      "Best Hyperparameters: {'C': 2.8101186312421578, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8761904761904763\n",
      "Best Hyperparameters: {'C': 0.09149222561428132, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8066666666666666\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9418181818181818\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8777777777777779\n",
      "Best Hyperparameters: {'C': 0.0002012896661878088, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.909090909090909\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8166666666666667\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9800000000000001\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8514285714285714\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8642857142857142\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9305555555555556\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9366666666666668\n",
      "Best Hyperparameters: {'C': 4.4627818740211815e-05, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.6794871794871795\n",
      "Best Hyperparameters: {'C': 0.0043963310508178515, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7464285714285714\n",
      "Best Hyperparameters: {'C': 0.007722748477617412, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9436363636363637\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8533333333333333\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8666666666666668\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.6871794871794872\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.890909090909091\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8714285714285713\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9358974358974359\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7694444444444445\n",
      "Best Hyperparameters: {'C': 0.0004493894149083743, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8571428571428571\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7975\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8555555555555555\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8716666666666667\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8285714285714286\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9111111111111111\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9454545454545455\n",
      "Best Hyperparameters: {'C': 0.48777805499545607, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.888888888888889\n",
      "Best Hyperparameters: {'C': 0.0004493894149083743, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9333333333333332\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9121212121212121\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9179487179487179\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9384615384615385\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.7821428571428571\n",
      "Best Hyperparameters: {'C': 0.0004493894149083743, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.5454545454545454\n",
      "Best Hyperparameters: {'C': 1.876650861993697e-05, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8133333333333335\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7127272727272729\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8428571428571429\n",
      "Best Hyperparameters: {'C': 3.522289925286184, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8755555555555556\n",
      "Best Hyperparameters: {'C': 0.007722748477617412, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8227272727272729\n",
      "Best Hyperparameters: {'C': 0.9793160285030893, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9083333333333332\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8833333333333332\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.925\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9217948717948719\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7875\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8321428571428571\n",
      "Best Hyperparameters: {'C': 0.008301451461243866, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9179487179487179\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.6523809523809524\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.65\n",
      "Best Hyperparameters: {'C': 0.0013071577689307433, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.8238095238095239\n",
      "Best Hyperparameters: {'C': 2.8101186312421578, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "For topic  R175 the classifier needs samples of at least 2 classes in the data but the data contains only one class: 1\n",
      "For topic  R175 the classifier needs samples of at least 2 classes in the data but the data contains only one class: 1\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "IRmodels=[]\n",
    "IRmodels_names=[]\n",
    "classifiers=[]\n",
    "classifiers_names=[]\n",
    "'''\n",
    "Ranking by Probability\n",
    "'''\n",
    "hyper_parameters=None\n",
    "\n",
    "l_aidedRanked,l_aidedNonRanked,l_nonAided,l_classifier_metrics=evaluate(Qtest,DTest,\n",
    "    trainX=trainX,testX=testX,RTrain=RTrain,RTest=RTest,k=k,classifier_type='logistic',ranking_type='proba',\n",
    "    hyper_parameters=hyper_parameters)\n",
    "\n",
    "IRmodels+=l_nonAided,l_aidedNonRanked,l_aidedRanked\n",
    "IRmodels_names+='Not Aided',' Logistic Aided Not Ranked without Hyper Paremeter Tuning','Logistic Aided Ranked By Probability without Hyper Parameter Tuning'\n",
    "classifiers.append(l_classifier_metrics)\n",
    "classifiers_names.append('Logistic Classifier without Hyper Parametrization')\n",
    "'''\n",
    "Hyper parameter Tuning\n",
    "'''\n",
    "hyper_parameters=hyper_parameter_search('logistic',trainX,RTrain)\n",
    "\n",
    "l_aidedRanked,l_aidedNonRanked,l_nonAided,l_classifier_metrics=evaluate(Qtest,DTest,\n",
    "    trainX=trainX,testX=testX,RTrain=RTrain,RTest=RTest,k=k,classifier_type='logistic',ranking_type='proba',\n",
    "    hyper_parameters=hyper_parameters)\n",
    "\n",
    "IRmodels+=l_aidedNonRanked,l_aidedRanked\n",
    "IRmodels_names+='Logistic Aided Not Ranked with Hyper Paremeter Tuning','Logistic Aided Ranked By Probability with Hyper Parameter Tuning'\n",
    "classifiers.append(l_classifier_metrics)\n",
    "classifiers_names.append('Logistic Classifier with Hyper Parametrization')\n",
    "\n",
    "'''\n",
    "Ranked by Score\n",
    "'''\n",
    "x_aidedRanked,x_aidedNonRanked,x_nonAided,x_classifier_metrics=evaluate(Qtest,DTest,\n",
    "    trainX=trainX,testX=testX,RTrain=RTrain,RTest=RTest,k=k,classifier_type='XGBOOST',ranking_type='score')\n",
    "\n",
    "IRmodels+=x_aidedNonRanked,x_aidedRanked\n",
    "IRmodels_names+='XGBoost Aided Not Ranked', 'XGBoost Aided Ranked By Probability'\n",
    "classifiers.append(x_classifier_metrics)\n",
    "classifiers_names.append('XGBOOST Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Classifier without Hyper Parametrization  TOPIC  R101\n",
      "Precision: 0.709397535151822  Recall: 0.7094884787067197  F-Score 0.7094408578157181  AP: 0.7520561451823972 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R101\n",
      "Precision: 0.709397535151822  Recall: 0.7094884787067197  F-Score 0.7094408578157181  AP: 0.7520561451823972 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R101\n",
      "Precision: 0.6796674116261745  Recall: 0.6803957051514055  F-Score 0.6791293552527558  AP: 0.6726361101190428 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R102\n",
      "Precision: 0.7138655462184874  Recall: 0.6503946646405808  F-Score 0.6280321590007246  AP: 0.6881654475455521 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R102\n",
      "Precision: 0.7138655462184874  Recall: 0.6503946646405808  F-Score 0.6280321590007246  AP: 0.6881654475455521 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R102\n",
      "Precision: 0.6049973418394472  Recall: 0.5917014900173062  F-Score 0.5815217391304348  AP: 0.6387073765628998 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R103\n",
      "Precision: 0.7071428571428571  Recall: 0.6353950924983326  F-Score 0.6600487634970393  AP: 0.41381171733424366 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R103\n",
      "Precision: 0.7071428571428571  Recall: 0.6353950924983326  F-Score 0.6600487634970393  AP: 0.41381171733424366 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R103\n",
      "Precision: 0.6012731481481481  Recall: 0.6474356724119774  F-Score 0.6146886491005575  AP: 0.2513502453448971 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R104\n",
      "Precision: 0.6687538651824366  Recall: 0.6882978723404255  F-Score 0.6530622930219774  AP: 0.5496524087115348 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R104\n",
      "Precision: 0.6687538651824366  Recall: 0.6882978723404255  F-Score 0.6530622930219774  AP: 0.5496524087115348 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R104\n",
      "Precision: 0.6894167012018235  Recall: 0.7102645198389879  F-Score 0.689071793976664  AP: 0.5735040899155422 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R105\n",
      "Precision: 0.6402116402116402  Recall: 0.6223076923076922  F-Score 0.6296144380639869  AP: 0.438343988860485 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R105\n",
      "Precision: 0.6402116402116402  Recall: 0.6223076923076922  F-Score 0.6296144380639869  AP: 0.438343988860485 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R105\n",
      "Precision: 0.60775956284153  Recall: 0.6422115384615384  F-Score 0.6146496163682864  AP: 0.544351333631033 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R106\n",
      "Precision: 0.6229773462783171  Recall: 0.5507230255839821  F-Score 0.563807896882401  AP: 0.276988664569846 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R106\n",
      "Precision: 0.6229773462783171  Recall: 0.5507230255839821  F-Score 0.563807896882401  AP: 0.276988664569846 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R106\n",
      "Precision: 0.6176900584795322  Recall: 0.6343159065628476  F-Score 0.6250486696950032  AP: 0.22379645906278922 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R107\n",
      "Precision: 0.5939165186500888  Recall: 0.5214090494989372  F-Score 0.5257571153651371  AP: 0.18310815938388866 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R107\n",
      "Precision: 0.5939165186500888  Recall: 0.5214090494989372  F-Score 0.5257571153651371  AP: 0.18310815938388866 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R107\n",
      "Precision: 0.967600700525394  Recall: 0.5  F-Score 0.4832579185520362  AP: 0.128788493468665 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R108\n",
      "Precision: 0.7712024123633623  Recall: 0.6292902066486972  F-Score 0.6724848484848485  AP: 0.28532155982939017 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R108\n",
      "Precision: 0.7712024123633623  Recall: 0.6292902066486972  F-Score 0.6724848484848485  AP: 0.28532155982939017 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R108\n",
      "Precision: 0.6257067470787787  Recall: 0.5599281221922732  F-Score 0.5789090909090909  AP: 0.21455863054409366 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R109\n",
      "Precision: 0.6952208864960543  Recall: 0.7154835558450017  F-Score 0.635672035036904  AP: 0.7174006460967275 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R109\n",
      "Precision: 0.6952208864960543  Recall: 0.7154835558450017  F-Score 0.635672035036904  AP: 0.7174006460967275 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R109\n",
      "Precision: 0.6804159312408871  Recall: 0.6913871703028329  F-Score 0.6036092421634591  AP: 0.48169025138854316 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R110\n",
      "Precision: 0.5597915051262169  Recall: 0.5973352033660589  F-Score 0.5703945275214762  AP: 0.14439591291301246 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R110\n",
      "Precision: 0.5597915051262169  Recall: 0.5973352033660589  F-Score 0.5703945275214762  AP: 0.14439591291301246 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R110\n",
      "Precision: 0.5469396195202647  Recall: 0.5875525946704068  F-Score 0.5545919663267898  AP: 0.14554630959049628 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R111\n",
      "Precision: 0.5396763854425145  Recall: 0.6173547400611621  F-Score 0.5477798588644834  AP: 0.11496502861132249 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R111\n",
      "Precision: 0.5396763854425145  Recall: 0.6173547400611621  F-Score 0.5477798588644834  AP: 0.11496502861132249 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R111\n",
      "Precision: 0.983370288248337  Recall: 0.5  F-Score 0.4915445321307779  AP: 0.12558154320546136 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R112\n",
      "Precision: 0.6162395900064062  Recall: 0.6968004338394794  F-Score 0.6432394507366944  AP: 0.30341998761540784 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R112\n",
      "Precision: 0.6162395900064062  Recall: 0.6968004338394794  F-Score 0.6432394507366944  AP: 0.30341998761540784 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R112\n",
      "Precision: 0.6162395900064062  Recall: 0.6968004338394794  F-Score 0.6432394507366944  AP: 0.1695842169443198 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R113\n",
      "Precision: 0.6815476190476191  Recall: 0.6301719027860107  F-Score 0.6489153229965277  AP: 0.4356032853501879 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R113\n",
      "Precision: 0.6815476190476191  Recall: 0.6301719027860107  F-Score 0.6489153229965277  AP: 0.4356032853501879 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R113\n",
      "Precision: 0.665577960806589  Recall: 0.620954356846473  F-Score 0.6373842938190504  AP: 0.31596976886272393 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R114\n",
      "Precision: 0.7427657630216266  Recall: 0.7579566296256338  F-Score 0.7498856164294023  AP: 0.5611492819663282 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R114\n",
      "Precision: 0.7427657630216266  Recall: 0.7579566296256338  F-Score 0.7498856164294023  AP: 0.5611492819663282 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R114\n",
      "Precision: 0.7432251340083382  Recall: 0.7643489049519905  F-Score 0.7528706681649682  AP: 0.43836421525398667 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R115\n",
      "Precision: 0.8471428571428572  Recall: 0.5459183673469388  F-Score 0.5406832298136647  AP: 0.4290069572102296 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R115\n",
      "Precision: 0.8471428571428572  Recall: 0.5459183673469388  F-Score 0.5406832298136647  AP: 0.4290069572102296 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R115\n",
      "Precision: 0.41151685393258425  Recall: 0.49829931972789115  F-Score 0.45076923076923076  AP: 0.5261236765453801 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R116\n",
      "Precision: 0.4676037644787645  Recall: 0.4707468540611211  F-Score 0.4681230813164846  AP: 0.29272055246117945 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R116\n",
      "Precision: 0.4676037644787645  Recall: 0.4707468540611211  F-Score 0.4681230813164846  AP: 0.29272055246117945 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R116\n",
      "Precision: 0.45909407665505225  Recall: 0.452034646184017  F-Score 0.44855662472242785  AP: 0.2774900537653178 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R117\n",
      "Precision: 0.44920369894682766  Recall: 0.4067216981132076  F-Score 0.42283400809716604  AP: 0.11616280319721428 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R117\n",
      "Precision: 0.44920369894682766  Recall: 0.4067216981132076  F-Score 0.42283400809716604  AP: 0.11616280319721428 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R117\n",
      "Precision: 0.5025252525252525  Recall: 0.5058372641509434  F-Score 0.4662588824955072  AP: 0.1415131706733158 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R118\n",
      "Precision: 0.9860627177700348  Recall: 0.7142857142857143  F-Score 0.7929328621908127  AP: 0.7672186418427019 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R118\n",
      "Precision: 0.9860627177700348  Recall: 0.7142857142857143  F-Score 0.7929328621908127  AP: 0.7672186418427019 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R118\n",
      "Precision: 0.47594501718213056  Recall: 0.496415770609319  F-Score 0.4859649122807018  AP: 0.18278545574149557 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R119\n",
      "Precision: 0.49742857142857144  Recall: 0.498538961038961  F-Score 0.49200777069629525  AP: 0.18269791136549268 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R119\n",
      "Precision: 0.49742857142857144  Recall: 0.498538961038961  F-Score 0.49200777069629525  AP: 0.18269791136549268 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R119\n",
      "Precision: 0.5904344193817879  Recall: 0.5468614718614718  F-Score 0.5530406709478191  AP: 0.2753849873017793 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R120\n",
      "Precision: 0.8096385542168675  Recall: 0.5  F-Score 0.3824404761904762  AP: 0.43857162296065416 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R120\n",
      "Precision: 0.8096385542168675  Recall: 0.5  F-Score 0.3824404761904762  AP: 0.43857162296065416 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R120\n",
      "Precision: 0.6360273972602739  Recall: 0.5611362852780377  F-Score 0.5316751174870146  AP: 0.4719164248929567 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R121\n",
      "Precision: 0.8001007472084627  Recall: 0.6659008632692843  F-Score 0.7048240338280114  AP: 0.5453723008596922 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R121\n",
      "Precision: 0.8001007472084627  Recall: 0.6659008632692843  F-Score 0.7048240338280114  AP: 0.5453723008596922 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R121\n",
      "Precision: 0.6981622990014533  Recall: 0.6961849067112225  F-Score 0.6971645802844165  AP: 0.3720507752451142 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R122\n",
      "Precision: 0.584780178837556  Recall: 0.6304609563123496  F-Score 0.5935974558838246  AP: 0.3287594386382642 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R122\n",
      "Precision: 0.584780178837556  Recall: 0.6304609563123496  F-Score 0.5935974558838246  AP: 0.3287594386382642 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R122\n",
      "Precision: 0.5898395721925134  Recall: 0.6348469212246302  F-Score 0.5998868778280544  AP: 0.22735012111394826 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R123\n",
      "Precision: 0.649671052631579  Recall: 0.8129411764705883  F-Score 0.6935395288336464  AP: 0.539367644290918 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R123\n",
      "Precision: 0.649671052631579  Recall: 0.8129411764705883  F-Score 0.6935395288336464  AP: 0.539367644290918 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R123\n",
      "Precision: 0.6621975806451612  Recall: 0.7912217194570136  F-Score 0.7032299533986823  AP: 0.30251708971549907 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R124\n",
      "Precision: 0.644880174291939  Recall: 0.6485826001955034  F-Score 0.6466857398917651  AP: 0.43495902853830243 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R124\n",
      "Precision: 0.644880174291939  Recall: 0.6485826001955034  F-Score 0.6466857398917651  AP: 0.43495902853830243 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R124\n",
      "Precision: 0.5771428571428572  Recall: 0.6413908671973187  F-Score 0.5748299319727891  AP: 0.2433277553237673 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R125\n",
      "Precision: 0.7627450980392156  Recall: 0.5837746395998823  F-Score 0.5876956851266236  AP: 0.4848320318091792 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R125\n",
      "Precision: 0.7627450980392156  Recall: 0.5837746395998823  F-Score 0.5876956851266236  AP: 0.4848320318091792 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R125\n",
      "Precision: 0.628125  Recall: 0.5723742277140336  F-Score 0.5768280406332937  AP: 0.34524269399268165 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R126\n",
      "Precision: 0.5634920634920635  Recall: 0.5491219743711437  F-Score 0.5442403185892919  AP: 0.5386355766581017 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R126\n",
      "Precision: 0.5634920634920635  Recall: 0.5491219743711437  F-Score 0.5442403185892919  AP: 0.5386355766581017 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R126\n",
      "Precision: 0.5533333333333333  Recall: 0.5462743236829616  F-Score 0.5449748783292121  AP: 0.6638510537688521 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R127\n",
      "Precision: 0.6600996376811594  Recall: 0.6717687074829932  F-Score 0.6654170571696345  AP: 0.47818542794484437 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R127\n",
      "Precision: 0.6600996376811594  Recall: 0.6717687074829932  F-Score 0.6654170571696345  AP: 0.47818542794484437 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R127\n",
      "Precision: 0.61489898989899  Recall: 0.5552721088435374  F-Score 0.5613782051282051  AP: 0.32589720374743925 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R128\n",
      "Precision: 0.5987130739982451  Recall: 0.5420875420875421  F-Score 0.5494071146245059  AP: 0.24012718097107866 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R128\n",
      "Precision: 0.5987130739982451  Recall: 0.5420875420875421  F-Score 0.5494071146245059  AP: 0.24012718097107866 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R128\n",
      "Precision: 0.5987130739982451  Recall: 0.5420875420875421  F-Score 0.5494071146245059  AP: 0.18825072119504668 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R129\n",
      "Precision: 0.6244444444444445  Recall: 0.6244444444444445  F-Score 0.6244444444444445  AP: 0.2852199971161439 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R129\n",
      "Precision: 0.6244444444444445  Recall: 0.6244444444444445  F-Score 0.6244444444444445  AP: 0.2852199971161439 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R129\n",
      "Precision: 0.5398798661800487  Recall: 0.561345029239766  F-Score 0.5420054200542006  AP: 0.16023447688634634 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R130\n",
      "Precision: 0.778476821192053  Recall: 0.5903135738831615  F-Score 0.6302095880510721  AP: 0.5249200223097548 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R130\n",
      "Precision: 0.778476821192053  Recall: 0.5903135738831615  F-Score 0.6302095880510721  AP: 0.5249200223097548 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R130\n",
      "Precision: 0.9739413680781759  Recall: 0.5  F-Score 0.4866220735785953  AP: 0.5368553203040174 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R131\n",
      "Precision: 0.47708333333333336  Recall: 0.4949893713938658  F-Score 0.43918994102592634  AP: 0.24832164633957104 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R131\n",
      "Precision: 0.47708333333333336  Recall: 0.4949893713938658  F-Score 0.43918994102592634  AP: 0.24832164633957104 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R131\n",
      "Precision: 0.46684905053105885  Recall: 0.49218038262982083  F-Score 0.4373604564624162  AP: 0.295297034919061 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R132\n",
      "Precision: 0.551060606060606  Recall: 0.6083833619210978  F-Score 0.5618563685636857  AP: 0.1458071653595966 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R132\n",
      "Precision: 0.551060606060606  Recall: 0.6083833619210978  F-Score 0.5618563685636857  AP: 0.1458071653595966 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R132\n",
      "Precision: 0.5437796619207593  Recall: 0.5880145797598628  F-Score 0.5523271171218765  AP: 0.09309925819632853 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R133\n",
      "Precision: 0.7551510989010989  Recall: 0.6507711038961039  F-Score 0.686389029964449  AP: 0.5828983052411485 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R133\n",
      "Precision: 0.7551510989010989  Recall: 0.6507711038961039  F-Score 0.686389029964449  AP: 0.5828983052411485 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R133\n",
      "Precision: 0.7105263157894737  Recall: 0.6465097402597403  F-Score 0.6711527558115246  AP: 0.37245375115692947 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R134\n",
      "Precision: 0.7153955808980755  Recall: 0.6588185831406348  F-Score 0.6783058130624878  AP: 0.4996934732585897 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R134\n",
      "Precision: 0.7153955808980755  Recall: 0.6588185831406348  F-Score 0.6783058130624878  AP: 0.4996934732585897 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R134\n",
      "Precision: 0.6163995578889196  Recall: 0.5442768551608156  F-Score 0.5432116868596147  AP: 0.21533908392110035 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R135\n",
      "Precision: 0.5631400966183575  Recall: 0.5709452124194833  F-Score 0.5380621158201229  AP: 0.7472388433735009 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R135\n",
      "Precision: 0.5631400966183575  Recall: 0.5709452124194833  F-Score 0.5380621158201229  AP: 0.7472388433735009 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R135\n",
      "Precision: 0.566161468959487  Recall: 0.573930665122675  F-Score 0.5625598434263973  AP: 0.7098115889990113 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R136\n",
      "Precision: 0.5590270640630353  Recall: 0.5333979453382438  F-Score 0.5358662168109138  AP: 0.2715859665442233 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R136\n",
      "Precision: 0.5590270640630353  Recall: 0.5333979453382438  F-Score 0.5358662168109138  AP: 0.2715859665442233 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R136\n",
      "Precision: 0.5870336217903248  Recall: 0.5713510370226788  F-Score 0.5772108843537415  AP: 0.2613192496108472 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R137\n",
      "Precision: 0.7209033005211349  Recall: 0.7682841068917019  F-Score 0.7420634920634921  AP: 0.5643801041650505 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R137\n",
      "Precision: 0.7209033005211349  Recall: 0.7682841068917019  F-Score 0.7420634920634921  AP: 0.5643801041650505 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R137\n",
      "Precision: 0.7209033005211349  Recall: 0.7682841068917019  F-Score 0.7420634920634921  AP: 0.3855744949494949 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R138\n",
      "Precision: 0.6053763440860215  Recall: 0.5470550576184379  F-Score 0.554686651460845  AP: 0.4273546920627187 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R138\n",
      "Precision: 0.6053763440860215  Recall: 0.5470550576184379  F-Score 0.554686651460845  AP: 0.4273546920627187 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R138\n",
      "Precision: 0.6829977628635346  Recall: 0.6309218950064021  F-Score 0.6496702888455466  AP: 0.2900426683443189 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R139\n",
      "Precision: 0.8450413223140496  Recall: 0.728938185443669  F-Score 0.7731619844590556  AP: 0.6438633165543556 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R139\n",
      "Precision: 0.8450413223140496  Recall: 0.728938185443669  F-Score 0.7731619844590556  AP: 0.6438633165543556 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R139\n",
      "Precision: 0.9664031620553359  Recall: 0.5  F-Score 0.48261758691206547  AP: 0.4836241728824849 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R140\n",
      "Precision: 0.7589285714285714  Recall: 0.666019218973625  F-Score 0.6959962468672454  AP: 0.5378403257181005 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R140\n",
      "Precision: 0.7589285714285714  Recall: 0.666019218973625  F-Score 0.6959962468672454  AP: 0.5378403257181005 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R140\n",
      "Precision: 0.6593675087589894  Recall: 0.6413616847270497  F-Score 0.649255751014885  AP: 0.37396114592856555 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R141\n",
      "Precision: 0.6032067510548523  Recall: 0.6004352467767102  F-Score 0.6017457889863396  AP: 0.35528328870736736 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R141\n",
      "Precision: 0.6032067510548523  Recall: 0.6004352467767102  F-Score 0.6017457889863396  AP: 0.35528328870736736 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R141\n",
      "Precision: 0.6232835939360266  Recall: 0.6275560482877556  F-Score 0.6252824858757062  AP: 0.3581528148879152 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R142\n",
      "Precision: 0.4491940420322383  Recall: 0.4403735632183908  F-Score 0.4445239012046867  AP: 0.17398827292379873 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R142\n",
      "Precision: 0.4491940420322383  Recall: 0.4403735632183908  F-Score 0.4445239012046867  AP: 0.17398827292379873 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R142\n",
      "Precision: 0.9393939393939394  Recall: 0.5  F-Score 0.46774193548387094  AP: 0.18157493183942586 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R143\n",
      "Precision: 0.9724220623501199  Recall: 0.5  F-Score 0.48581997533908755  AP: 0.16141879192539355 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R143\n",
      "Precision: 0.9724220623501199  Recall: 0.5  F-Score 0.48581997533908755  AP: 0.16141879192539355 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R143\n",
      "Precision: 0.9724220623501199  Recall: 0.5  F-Score 0.48581997533908755  AP: 0.0594946463794529 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R144\n",
      "Precision: 0.739448051948052  Recall: 0.698041958041958  F-Score 0.7154601995751768  AP: 0.4707990134698886 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R144\n",
      "Precision: 0.739448051948052  Recall: 0.698041958041958  F-Score 0.7154601995751768  AP: 0.4707990134698886 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R144\n",
      "Precision: 0.7526075619295958  Recall: 0.6300699300699301  F-Score 0.6614483899707266  AP: 0.40096854861573444 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R145\n",
      "Precision: 0.4720496894409938  Recall: 0.4945770065075922  F-Score 0.4830508474576271  AP: 0.07215346139961365 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R145\n",
      "Precision: 0.4720496894409938  Recall: 0.4945770065075922  F-Score 0.4830508474576271  AP: 0.07215346139961365 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R145\n",
      "Precision: 0.4720496894409938  Recall: 0.4945770065075922  F-Score 0.4830508474576271  AP: 0.07942176980359288 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R146\n",
      "Precision: 0.8528464017185822  Recall: 0.8677434831280986  F-Score 0.8547416476447396  AP: 0.9489346287463942 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R146\n",
      "Precision: 0.8528464017185822  Recall: 0.8677434831280986  F-Score 0.8547416476447396  AP: 0.9489346287463942 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R146\n",
      "Precision: 0.6777721971092137  Recall: 0.6698118236579775  F-Score 0.6723809523809524  AP: 0.6759735773231705 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R147\n",
      "Precision: 0.6008939974457216  Recall: 0.5873002380142809  F-Score 0.5932682015895789  AP: 0.26482461897007514 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R147\n",
      "Precision: 0.6008939974457216  Recall: 0.5873002380142809  F-Score 0.5932682015895789  AP: 0.26482461897007514 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R147\n",
      "Precision: 0.6695906432748537  Recall: 0.6873512410744644  F-Score 0.6778100775193798  AP: 0.23937912870133687 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R148\n",
      "Precision: 0.750140056022409  Recall: 0.7576754385964912  F-Score 0.7521200260926288  AP: 0.8630118873843251 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R148\n",
      "Precision: 0.750140056022409  Recall: 0.7576754385964912  F-Score 0.7521200260926288  AP: 0.8630118873843251 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R148\n",
      "Precision: 0.6126732409381663  Recall: 0.5975877192982456  F-Score 0.5476190476190477  AP: 0.7444757837452487 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R149\n",
      "Precision: 0.6045899172310008  Recall: 0.5124418188327963  F-Score 0.4964166904286665  AP: 0.28416263192706503 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R149\n",
      "Precision: 0.6045899172310008  Recall: 0.5124418188327963  F-Score 0.4964166904286665  AP: 0.28416263192706503 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R149\n",
      "Precision: 0.43609865470852016  Recall: 0.4961734693877551  F-Score 0.46420047732696895  AP: 0.17517861355430533 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R150\n",
      "Precision: 0.6827880512091038  Recall: 0.6351209253417455  F-Score 0.6525752508361204  AP: 0.42308143805105597 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R150\n",
      "Precision: 0.6827880512091038  Recall: 0.6351209253417455  F-Score 0.6525752508361204  AP: 0.42308143805105597 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R150\n",
      "Precision: 0.7526198439241918  Recall: 0.6323752774856876  F-Score 0.6637462235649547  AP: 0.37955575911357453 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R151\n",
      "Precision: 0.6444704049844237  Recall: 0.5609529025191675  F-Score 0.5819461982933456  AP: 0.12660190209555044 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R151\n",
      "Precision: 0.6444704049844237  Recall: 0.5609529025191675  F-Score 0.5819461982933456  AP: 0.12660190209555044 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R151\n",
      "Precision: 0.5380244755244755  Recall: 0.5142935377875137  F-Score 0.516745655608215  AP: 0.08143300453177568 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R152\n",
      "Precision: 0.5207956600361664  Recall: 0.5038848726437403  F-Score 0.4904100529100529  AP: 0.11518338579154733 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R152\n",
      "Precision: 0.5207956600361664  Recall: 0.5038848726437403  F-Score 0.4904100529100529  AP: 0.11518338579154733 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R152\n",
      "Precision: 0.7  Recall: 0.510810080399973  F-Score 0.49631757479448707  AP: 0.11333612273806684 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R153\n",
      "Precision: 0.6666666666666667  Recall: 0.6908575241908576  F-Score 0.6662844913246198  AP: 0.6185655293851715 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R153\n",
      "Precision: 0.6666666666666667  Recall: 0.6908575241908576  F-Score 0.6662844913246198  AP: 0.6185655293851715 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R153\n",
      "Precision: 0.6838578088578089  Recall: 0.7105438772105439  F-Score 0.6843231674692349  AP: 0.4636438916099933 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R154\n",
      "Precision: 0.5828378378378378  Recall: 0.554830053667263  F-Score 0.5640017162471396  AP: 0.13578836170631475 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R154\n",
      "Precision: 0.5828378378378378  Recall: 0.554830053667263  F-Score 0.5640017162471396  AP: 0.13578836170631475 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R154\n",
      "Precision: 0.5828378378378378  Recall: 0.554830053667263  F-Score 0.5640017162471396  AP: 0.12929547146563386 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R155\n",
      "Precision: 0.4354508196721312  Recall: 0.4988262910798122  F-Score 0.4649890590809628  AP: 0.17730877608160328 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R155\n",
      "Precision: 0.4354508196721312  Recall: 0.4988262910798122  F-Score 0.4649890590809628  AP: 0.17730877608160328 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R155\n",
      "Precision: 0.5185842713605802  Recall: 0.518332215515314  F-Score 0.5184525205158265  AP: 0.1563732122069472 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R156\n",
      "Precision: 0.9124270790937458  Recall: 0.9491725768321513  F-Score 0.9291291291291291  AP: 0.953267502462907 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R156\n",
      "Precision: 0.9124270790937458  Recall: 0.9491725768321513  F-Score 0.9291291291291291  AP: 0.953267502462907 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R156\n",
      "Precision: 0.8208029197080291  Recall: 0.8463356973995272  F-Score 0.8324498296099961  AP: 0.6615708576486463 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R157\n",
      "Precision: 0.5641891891891893  Recall: 0.5078100914602816  F-Score 0.4895065229722065  AP: 0.20530755909764187 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R157\n",
      "Precision: 0.5641891891891893  Recall: 0.5078100914602816  F-Score 0.4895065229722065  AP: 0.20530755909764187 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R157\n",
      "Precision: 0.43791946308724833  Recall: 0.49619771863117873  F-Score 0.46524064171123  AP: 0.19723345111837604 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R158\n",
      "Precision: 0.5213014981273408  Recall: 0.5040688575899843  F-Score 0.49413465585710886  AP: 0.2742477267538063 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R158\n",
      "Precision: 0.5213014981273408  Recall: 0.5040688575899843  F-Score 0.49413465585710886  AP: 0.2742477267538063 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R158\n",
      "Precision: 0.7100371747211895  Recall: 0.5202101497876146  F-Score 0.5190771960958296  AP: 0.224551288857932 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R159\n",
      "Precision: 0.5875139353400223  Recall: 0.5686841404496519  F-Score 0.5721834707250053  AP: 0.35320528102345683 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R159\n",
      "Precision: 0.5875139353400223  Recall: 0.5686841404496519  F-Score 0.5721834707250053  AP: 0.35320528102345683 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R159\n",
      "Precision: 0.5428949357520786  Recall: 0.5431772358960703  F-Score 0.54303047537798  AP: 0.3212726582798979 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R160\n",
      "Precision: 0.6916387959866221  Recall: 0.7195402298850575  F-Score 0.6999556541019956  AP: 0.49274495769276333 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R160\n",
      "Precision: 0.6916387959866221  Recall: 0.7195402298850575  F-Score 0.6999556541019956  AP: 0.49274495769276333 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R160\n",
      "Precision: 0.6850373812891494  Recall: 0.7339080459770115  F-Score 0.6700363714163458  AP: 0.5729247630502635 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R161\n",
      "Precision: 0.7897233201581028  Recall: 0.6499590834697218  F-Score 0.6909212283044058  AP: 0.48781491050611436 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R161\n",
      "Precision: 0.7897233201581028  Recall: 0.6499590834697218  F-Score 0.6909212283044058  AP: 0.48781491050611436 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R161\n",
      "Precision: 0.7651061173533085  Recall: 0.6086078150572831  F-Score 0.644259805235415  AP: 0.47164495604096385 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R162\n",
      "Precision: 0.5883384932920537  Recall: 0.5444029463637307  F-Score 0.5385651342588743  AP: 0.36717474353302226 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R162\n",
      "Precision: 0.5883384932920537  Recall: 0.5444029463637307  F-Score 0.5385651342588743  AP: 0.36717474353302226 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R162\n",
      "Precision: 0.6129234972677596  Recall: 0.6071947297437493  F-Score 0.6096925204809023  AP: 0.326241008531527 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R163\n",
      "Precision: 0.6963109354413702  Recall: 0.5994733328388102  F-Score 0.5900900900900901  AP: 0.6327142427158823 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R163\n",
      "Precision: 0.6963109354413702  Recall: 0.5994733328388102  F-Score 0.5900900900900901  AP: 0.6327142427158823 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R163\n",
      "Precision: 0.7001984126984127  Recall: 0.6309806394184407  F-Score 0.6336300372228516  AP: 0.554904037147644 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R164\n",
      "Precision: 0.7633011099176512  Recall: 0.7424395604395604  F-Score 0.7472033633497145  AP: 0.7612645031164186 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R164\n",
      "Precision: 0.7633011099176512  Recall: 0.7424395604395604  F-Score 0.7472033633497145  AP: 0.7612645031164186 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R164\n",
      "Precision: 0.6912698412698413  Recall: 0.6906813186813187  F-Score 0.6909599172371288  AP: 0.5708029942613579 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R165\n",
      "Precision: 0.5498956158663884  Recall: 0.5205644467389434  F-Score 0.5209983201343893  AP: 0.16796933770867772 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R165\n",
      "Precision: 0.5498956158663884  Recall: 0.5205644467389434  F-Score 0.5209983201343893  AP: 0.16796933770867772 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R165\n",
      "Precision: 0.4815382134795227  Recall: 0.48029599036310444  F-Score 0.4808572617561382  AP: 0.10711055747641827 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R166\n",
      "Precision: 0.6266637089618456  Recall: 0.6662783925451369  F-Score 0.6423366834170855  AP: 0.2532932209437972 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R166\n",
      "Precision: 0.6266637089618456  Recall: 0.6662783925451369  F-Score 0.6423366834170855  AP: 0.2532932209437972 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R166\n",
      "Precision: 0.6122419307938354  Recall: 0.7248107163657542  F-Score 0.6361262758129599  AP: 0.21112457823964073 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R167\n",
      "Precision: 0.6904306220095694  Recall: 0.5557735426008968  F-Score 0.5757808009197164  AP: 0.1935560015036626 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R167\n",
      "Precision: 0.6904306220095694  Recall: 0.5557735426008968  F-Score 0.5757808009197164  AP: 0.1935560015036626 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R167\n",
      "Precision: 0.7048245614035088  Recall: 0.6570627802690583  F-Score 0.6767184035476719  AP: 0.22563646028265932 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R168\n",
      "Precision: 0.5860933101754806  Recall: 0.5732036461781331  F-Score 0.5775691699604744  AP: 0.8451438797657228 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R168\n",
      "Precision: 0.5860933101754806  Recall: 0.5732036461781331  F-Score 0.5775691699604744  AP: 0.8451438797657228 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R168\n",
      "Precision: 0.5621218233158531  Recall: 0.5627387075418852  F-Score 0.5624216166913694  AP: 0.8321993249944619 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R169\n",
      "Precision: 0.6673648931713447  Recall: 0.5364673664993154  F-Score 0.5439056356487549  AP: 0.21309688040047037 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R169\n",
      "Precision: 0.6673648931713447  Recall: 0.5364673664993154  F-Score 0.5439056356487549  AP: 0.21309688040047037 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R169\n",
      "Precision: 0.471821756225426  Recall: 0.4823368324965769  F-Score 0.4756696428571428  AP: 0.08297653596270169 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R170\n",
      "Precision: 0.5928627538996746  Recall: 0.5522378637712266  F-Score 0.5599353506330251  AP: 0.23843156382259373 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R170\n",
      "Precision: 0.5928627538996746  Recall: 0.5522378637712266  F-Score 0.5599353506330251  AP: 0.23843156382259373 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R170\n",
      "Precision: 0.6093465171518533  Recall: 0.6249605454201124  F-Score 0.6158667108021626  AP: 0.2693056759122181 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R171\n",
      "Precision: 0.6645408163265306  Recall: 0.5058191988451822  F-Score 0.46693195384003183  AP: 0.16963823841032627 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R171\n",
      "Precision: 0.6645408163265306  Recall: 0.5058191988451822  F-Score 0.46693195384003183  AP: 0.16963823841032627 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R171\n",
      "Precision: 0.4969931271477663  Recall: 0.49968422952002883  F-Score 0.46309334544628655  AP: 0.16238885146859883 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R172\n",
      "Precision: 0.4531963470319635  Recall: 0.49625  F-Score 0.4737470167064439  AP: 0.1641429746016383 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R172\n",
      "Precision: 0.4531963470319635  Recall: 0.49625  F-Score 0.4737470167064439  AP: 0.1641429746016383 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R172\n",
      "Precision: 0.5454073561400039  Recall: 0.5560060975609756  F-Score 0.5492456764381209  AP: 0.16995428167825785 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R173\n",
      "Precision: 0.6115875912408759  Recall: 0.561494368463395  F-Score 0.5584374999999999  AP: 0.7673915529637849 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R173\n",
      "Precision: 0.6115875912408759  Recall: 0.561494368463395  F-Score 0.5584374999999999  AP: 0.7673915529637849 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R173\n",
      "Precision: 0.6157718120805369  Recall: 0.5277554304102976  F-Score 0.492953611274222  AP: 0.7376553837667881 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R174\n",
      "Precision: 0.7028996450955222  Recall: 0.6161996194430029  F-Score 0.633138480145132  AP: 0.5183435903232412 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R174\n",
      "Precision: 0.7028996450955222  Recall: 0.6161996194430029  F-Score 0.633138480145132  AP: 0.5183435903232412 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R174\n",
      "Precision: 0.5911174785100286  Recall: 0.5206279190451479  F-Score 0.4945022628130769  AP: 0.4662894052866994 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R175\n",
      "Precision: 1.0  Recall: 0.031055900621118012  F-Score 0.06024096385542168  AP: 1.0 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R175\n",
      "Precision: 1.0  Recall: 0.031055900621118012  F-Score 0.06024096385542168  AP: 1.0 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R175\n",
      "Precision: 1.0  Recall: 0.031055900621118012  F-Score 0.06024096385542168  AP: 1.0 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R176\n",
      "Precision: 0.6500289351851851  Recall: 0.6124078624078624  F-Score 0.627226253298153  AP: 0.350397988091639 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R176\n",
      "Precision: 0.6500289351851851  Recall: 0.6124078624078624  F-Score 0.627226253298153  AP: 0.350397988091639 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R176\n",
      "Precision: 0.6685557373218042  Recall: 0.6042419424772366  F-Score 0.625674745096297  AP: 0.27546314663474364 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R177\n",
      "Precision: 0.6716666666666666  Recall: 0.7233498135137479  F-Score 0.6747833415782627  AP: 0.42372299626332544 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R177\n",
      "Precision: 0.6716666666666666  Recall: 0.7233498135137479  F-Score 0.6747833415782627  AP: 0.42372299626332544 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R177\n",
      "Precision: 0.7542857142857142  Recall: 0.7894873796513141  F-Score 0.7676955397543632  AP: 0.5462822480255698 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R178\n",
      "Precision: 0.9132841328413284  Recall: 0.5  F-Score 0.45252525252525255  AP: 0.1452293102425209 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R178\n",
      "Precision: 0.9132841328413284  Recall: 0.5  F-Score 0.45252525252525255  AP: 0.1452293102425209 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R178\n",
      "Precision: 0.5135338345864662  Recall: 0.501709726443769  F-Score 0.4682103610675039  AP: 0.16893965799836408 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R179\n",
      "Precision: 0.9686274509803922  Recall: 0.5  F-Score 0.48380566801619435  AP: 0.04253139348746349 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R179\n",
      "Precision: 0.9686274509803922  Recall: 0.5  F-Score 0.48380566801619435  AP: 0.04253139348746349 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R179\n",
      "Precision: 0.46819085487077533  Recall: 0.49267782426778245  F-Score 0.4801223241590214  AP: 0.07183406659257977 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R180\n",
      "Precision: 0.9154929577464789  Recall: 0.5  F-Score 0.45384615384615384  AP: 0.19900453916861338 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R180\n",
      "Precision: 0.9154929577464789  Recall: 0.5  F-Score 0.45384615384615384  AP: 0.19900453916861338 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R180\n",
      "Precision: 0.5745426829268292  Recall: 0.5191854990583804  F-Score 0.5057710613993336  AP: 0.20468671605186933 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R181\n",
      "Precision: 0.47767857142857145  Recall: 0.4872495446265938  F-Score 0.48241659152389543  AP: 0.05672120268575104 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R181\n",
      "Precision: 0.47767857142857145  Recall: 0.4872495446265938  F-Score 0.48241659152389543  AP: 0.05672120268575104 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R181\n",
      "Precision: 0.5297794117647059  Recall: 0.5354098360655737  F-Score 0.5321300840056558  AP: 0.04437176040970166 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R182\n",
      "Precision: 0.5538914490527393  Recall: 0.5526249999999999  F-Score 0.5532157085941947  AP: 0.3131823857090373 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R182\n",
      "Precision: 0.5538914490527393  Recall: 0.5526249999999999  F-Score 0.5532157085941947  AP: 0.3131823857090373 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R182\n",
      "Precision: 0.5140237324703344  Recall: 0.5195000000000001  F-Score 0.5035699714402284  AP: 0.26718021070389414 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R183\n",
      "Precision: 0.5682578972182932  Recall: 0.5685228133282847  F-Score 0.5683867424846327  AP: 0.45769024520327184 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R183\n",
      "Precision: 0.5682578972182932  Recall: 0.5685228133282847  F-Score 0.5683867424846327  AP: 0.45769024520327184 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R183\n",
      "Precision: 0.5036412631954034  Recall: 0.5038692730026505  F-Score 0.5028172240036647  AP: 0.3130037434040168 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R184\n",
      "Precision: 0.9819944598337951  Recall: 0.5  F-Score 0.49083215796897034  AP: 0.051029978651610124 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R184\n",
      "Precision: 0.9819944598337951  Recall: 0.5  F-Score 0.49083215796897034  AP: 0.051029978651610124 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R184\n",
      "Precision: 0.5332355816226784  Recall: 0.5751547303271441  F-Score 0.5401582193617591  AP: 0.06522084018979889 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R185\n",
      "Precision: 0.587493881546745  Recall: 0.5831202046035806  F-Score 0.5772965503046876  AP: 0.6440354889610439 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R185\n",
      "Precision: 0.587493881546745  Recall: 0.5831202046035806  F-Score 0.5772965503046876  AP: 0.6440354889610439 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R185\n",
      "Precision: 0.5271481681357558  Recall: 0.5263165542897  F-Score 0.5222709979514193  AP: 0.499042432691467 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R186\n",
      "Precision: 0.6214142283419392  Recall: 0.6257427213309567  F-Score 0.6226886010457089  AP: 0.768597646999079 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R186\n",
      "Precision: 0.6214142283419392  Recall: 0.6257427213309567  F-Score 0.6226886010457089  AP: 0.768597646999079 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R186\n",
      "Precision: 0.5996336643208329  Recall: 0.6023469994058229  F-Score 0.6005284064214194  AP: 0.7036872310095732 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R187\n",
      "Precision: 0.6023569023569023  Recall: 0.5899674459899379  F-Score 0.5955754999212722  AP: 0.23649386362294372 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R187\n",
      "Precision: 0.6023569023569023  Recall: 0.5899674459899379  F-Score 0.5955754999212722  AP: 0.23649386362294372 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R187\n",
      "Precision: 0.46600877192982454  Recall: 0.4873853211009174  F-Score 0.476457399103139  AP: 0.14300408617104932 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R188\n",
      "Precision: 0.8749028749028749  Recall: 0.8749028749028749  F-Score 0.8749028749028749  AP: 0.8321067492476263 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R188\n",
      "Precision: 0.8749028749028749  Recall: 0.8749028749028749  F-Score 0.8749028749028749  AP: 0.8321067492476263 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R188\n",
      "Precision: 0.9454828660436136  Recall: 0.5138888888888888  F-Score 0.49819671401220006  AP: 0.3537840147652381 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4856348470806302  Recall: 0.4894053315105947  F-Score 0.48437500000000006  AP: 0.2743094555821511 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R189\n",
      "Precision: 0.4856348470806302  Recall: 0.4894053315105947  F-Score 0.48437500000000006  AP: 0.2743094555821511 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R189\n",
      "Precision: 0.5886260236578709  Recall: 0.5832194121667806  F-Score 0.5856054265956041  AP: 0.2712027973087835 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R190\n",
      "Precision: 0.7879464285714286  Recall: 0.734873949579832  F-Score 0.7544391179290508  AP: 0.6656624338768464 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R190\n",
      "Precision: 0.7879464285714286  Recall: 0.734873949579832  F-Score 0.7544391179290508  AP: 0.6656624338768464 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R190\n",
      "Precision: 0.7256734006734007  Recall: 0.6251633986928105  F-Score 0.6415154826958106  AP: 0.5455166640155992 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R191\n",
      "Precision: 0.4737609329446064  Recall: 0.4939209726443769  F-Score 0.48363095238095233  AP: 0.08873504410371261 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R191\n",
      "Precision: 0.4737609329446064  Recall: 0.4939209726443769  F-Score 0.48363095238095233  AP: 0.08873504410371261 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R191\n",
      "Precision: 0.47337278106508873  Recall: 0.48632218844984804  F-Score 0.47976011994003  AP: 0.07561110868176156 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R192\n",
      "Precision: 0.9604904632152589  Recall: 0.5  F-Score 0.4794326241134752  AP: 0.0683087412762014 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R192\n",
      "Precision: 0.9604904632152589  Recall: 0.5  F-Score 0.4794326241134752  AP: 0.0683087412762014 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R192\n",
      "Precision: 0.9604904632152589  Recall: 0.5  F-Score 0.4794326241134752  AP: 0.07581104843917125 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R193\n",
      "Precision: 0.9813953488372094  Recall: 0.5  F-Score 0.49052132701421797  AP: 0.03385878445950864 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R193\n",
      "Precision: 0.9813953488372094  Recall: 0.5  F-Score 0.49052132701421797  AP: 0.03385878445950864 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R193\n",
      "Precision: 0.4811764705882353  Recall: 0.4939613526570048  F-Score 0.48748510131108463  AP: 0.0449590176484106 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R194\n",
      "Precision: 0.729193899782135  Recall: 0.6318516042780749  F-Score 0.6380578093306288  AP: 0.6006547439158287 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R194\n",
      "Precision: 0.729193899782135  Recall: 0.6318516042780749  F-Score 0.6380578093306288  AP: 0.6006547439158287 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R194\n",
      "Precision: 0.6290370976541189  Recall: 0.6317541221033868  F-Score 0.6302424910443648  AP: 0.44212112488579647 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R195\n",
      "Precision: 0.42938931297709926  Recall: 0.497787610619469  F-Score 0.4610655737704918  AP: 0.14355031845379634 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R195\n",
      "Precision: 0.42938931297709926  Recall: 0.497787610619469  F-Score 0.4610655737704918  AP: 0.14355031845379634 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R195\n",
      "Precision: 0.5011806375442739  Recall: 0.500717531690983  F-Score 0.49616858237547895  AP: 0.1484513067725704 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R196\n",
      "Precision: 0.944812362030905  Recall: 0.5  F-Score 0.4707943925233645  AP: 0.16942162314137055 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R196\n",
      "Precision: 0.944812362030905  Recall: 0.5  F-Score 0.4707943925233645  AP: 0.16942162314137055 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R196\n",
      "Precision: 0.5890454836643177  Recall: 0.5137965260545906  F-Score 0.5038745273076686  AP: 0.11474930332423865 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R197\n",
      "Precision: 0.6090909090909091  Recall: 0.5611111111111111  F-Score 0.5223800589654248  AP: 0.5943849085570495 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R197\n",
      "Precision: 0.6090909090909091  Recall: 0.5611111111111111  F-Score 0.5223800589654248  AP: 0.5943849085570495 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R197\n",
      "Precision: 0.5226763717805151  Recall: 0.51875  F-Score 0.5050405704450455  AP: 0.5129620496561551 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R198\n",
      "Precision: 0.715587044534413  Recall: 0.5256132756132756  F-Score 0.5311715481171548  AP: 0.25005034132180626 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R198\n",
      "Precision: 0.715587044534413  Recall: 0.5256132756132756  F-Score 0.5311715481171548  AP: 0.25005034132180626 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R198\n",
      "Precision: 0.7684599156118144  Recall: 0.6836219336219336  F-Score 0.7162393162393162  AP: 0.3509417537195315 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R199\n",
      "Precision: 0.696078431372549  Recall: 0.6972811671087533  F-Score 0.6965916133030365  AP: 0.6541629034202754 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R199\n",
      "Precision: 0.696078431372549  Recall: 0.6972811671087533  F-Score 0.6965916133030365  AP: 0.6541629034202754 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R199\n",
      "Precision: 0.6512605042016807  Recall: 0.6521883289124668  F-Score 0.6516422226812644  AP: 0.5997819837981496 \n",
      "\n",
      "Logistic Classifier without Hyper Parametrization  TOPIC  R200\n",
      "Precision: 0.8460144927536232  Recall: 0.5058139534883721  F-Score 0.4204878288906938  AP: 0.38241609898688167 \n",
      "\n",
      "Logistic Classifier with Hyper Parametrization  TOPIC  R200\n",
      "Precision: 0.8460144927536232  Recall: 0.5058139534883721  F-Score 0.4204878288906938  AP: 0.38241609898688167 \n",
      "\n",
      "XGBOOST Classifier  TOPIC  R200\n",
      "Precision: 0.5028559771521828  Recall: 0.5008523073176671  F-Score 0.45357938275532933  AP: 0.2951672822430559 \n",
      "\n",
      "---------- Classifiers Summary----------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c8d57c91703f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_metrics_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifiers_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/OneDrive/1Mes/1Sem/PRI/PRI_Proj_II/util.py\u001b[0m in \u001b[0;36mshow_metrics_classifiers\u001b[0;34m(classifiers, classifiers_names, search_topics)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------- Classifiers Summary----------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MAP:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MAP'"
     ]
    }
   ],
   "source": [
    "show_metrics_classifiers(classifiers,classifiers_names,Qtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_graph\n",
    "\n",
    "__@input:__\n",
    "\n",
    "__D__ list of document names\n",
    "\n",
    "__sim__ similarity criteria between docs ex: 'tfidf'\n",
    "\n",
    "__teta__ float minimum similarity to link documents\n",
    "\n",
    "__minDocFreq__ paramter for tfidf vectorizer\n",
    "\n",
    "__maxDocFreq__ paramter for tfidf vectorizer\n",
    "\n",
    "\n",
    "__@output__\n",
    "\n",
    "__graph__ dictionary representing an undirected graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from woosh_index import *\n",
    "\n",
    "from statistics import mean\n",
    "from woosh_index import *\n",
    "from whoosh import scoring\n",
    "from util import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "\n",
    "def build_graph(D, sim, teta, minDocFreq=2, maxDocFreq=0.9):\n",
    "\tvectorizer = None\n",
    "\tdocs = noPreprocessDocs(D)\n",
    "\tif (sim == 'tfidf'):\n",
    "\t\tvectorizer = TfidfVectorizer(min_df= minDocFreq, max_df=maxDocFreq, stop_words='english')\n",
    "\ttfidf = vectorizer.fit_transform(docs)\n",
    "\t\n",
    "\tpairWiseDocSim = tfidf * tfidf.T\n",
    "\tpairWiseDocSim = pairWiseDocSim.toarray()\n",
    "\n",
    "\tgraph = {\n",
    "\t\t'outlinks': {},\n",
    "\t\t'inlinks': {},\n",
    "\t\t'weights': {}\n",
    "\t}\n",
    "\t# the graph is already indirected\n",
    "\tfor i in range(len(pairWiseDocSim)):\n",
    "\t\tfor j in range(len(pairWiseDocSim[i])):\n",
    "\t\t\tif(i == j):\n",
    "\t\t\t\t#print(\"Same doc\")\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif(pairWiseDocSim[i][j] > teta):\n",
    "\n",
    "\t\t\t\tif(graph.get('outlinks').get(D[i]) == None):\n",
    "\t\t\t\t\tgraph['outlinks'][D[i]] = [] \n",
    "\t\t\t\tgraph['outlinks'][D[i]] += [D[j]]\n",
    "\n",
    "\t\t\t\tif(graph.get('inlinks').get(D[j]) == None):\n",
    "\t\t\t\t\tgraph['inlinks'][D[j]] = []\n",
    "\t\t\t\tgraph['inlinks'][D[j]] += [D[i]]\n",
    "\n",
    "\t\t\t\tif(graph.get('weights').get(D[i]) == None):\n",
    "\t\t\t\t\tgraph['weights'][D[i]] = {}\n",
    "\n",
    "\t\t\t\tgraph['weights'][D[i]][D[j]] = pairWiseDocSim[i][j]\n",
    "\n",
    "\tfor i in graph['inlinks'].keys():\n",
    "\t\tif(len(graph['inlinks'][i]) == 0):\n",
    "\t\t\tprint(\"Sink\")\n",
    "\n",
    "\treturn graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## undirected_page_rank\n",
    "\n",
    "__@input:__\n",
    "\n",
    "__q__ topic name\n",
    "\n",
    "__D__ list of document names\n",
    "\n",
    "__p__ limit of retrieved documents\n",
    "\n",
    "__varaint__ 'standard' for traditional page ranking, 'priors' for page ranking with priors\n",
    "\n",
    "__iterations__ number of iterations for page ranking\n",
    "\n",
    "__d__ damping factor\n",
    "\n",
    "\n",
    "__@output__\n",
    "\n",
    "__graph__ list of pair document name and page ranking value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undirected_page_rank(q, D, p, graph, variant='standard', priorWeighting = scoring.TF_IDF(), iterations = 10, d = 0.15):\n",
    "\toutDegrees = {}\n",
    "\tN = len(graph['outlinks'].keys())\n",
    "\tprestigeVector = {}\n",
    "\n",
    "\n",
    "\ttopicIndex=TopicIndex(\"topics.txt\",\"topicsindexdir\")\n",
    "\ttopics=topicIndex.topics\n",
    "\n",
    "\ttrainIndex = WooshDocumentIndex(True, \"trainIndex\", D)\n",
    "\tDids = list(map(lambda x: extractFileId(x)+\"newsML.xml\", D))\n",
    "\n",
    "\n",
    "\tfor u in graph['outlinks'].keys():\n",
    "\t\toutDegrees[u] = len(graph['outlinks'][u])\n",
    "\t\tprestigeVector[u] = float(1/N)\n",
    "\n",
    "\tif (variant == 'standard'):\n",
    "\t\tfor i in range(iterations):\n",
    "\t\t\ttempPrestigeVector = prestigeVector.copy() \n",
    "\t\t\tfor v in prestigeVector.keys():\n",
    "\t\t\t\tnewValue = sum([prestigeVector[u] / outDegrees[u] for u in graph['inlinks'][v]])\n",
    "\t\t\t\ttempPrestigeVector[v] = d/N + (1 - d) * newValue\n",
    "\t\t\tprestigeVector = tempPrestigeVector.copy()\n",
    "\n",
    "\n",
    "\telif (variant == 'priors'):\n",
    "\n",
    "\t\tpriors = trainIndex.rank(topics[q.lower()], weighting=priorWeighting, k=None)\n",
    "\t\tfillVector(Dids, priors)\n",
    "\t\tnormalizeDict(priors)\n",
    "\t\tfor i in range(iterations):\n",
    "\t\t\ttempPrestigeVector = prestigeVector.copy() \n",
    "\t\t\tfor v in prestigeVector.keys():\n",
    "\t\t\t\t# For all inlinks of v\n",
    "\t\t\t\tnewValue = 0\n",
    "\t\t\t\tfor inlinkV in graph['inlinks'][v]:\n",
    "\t\t\t\t\tweightSum = sum([graph['weights'][inlinkV][ininlinkV] for ininlinkV in graph['inlinks'][inlinkV]])\n",
    "\t\t\t\t\tnewValue += (prestigeVector[inlinkV] * graph['weights'][inlinkV][v] / weightSum)\n",
    "\t\t\t\ttempPrestigeVector[v] = d * priors[extractFileId(v)+\"newsML.xml\"] + (1 - d) * newValue\n",
    "\n",
    "\t\t\tprestigeVector = tempPrestigeVector.copy()\n",
    "\t\t\t\n",
    "\torderedPrestigeVector = sorted(prestigeVector.items(), key=lambda x: x[1], reverse=True)\n",
    "\torderedPrestigeVector = list(map(lambda x: (extractFileId(x[0]) + \"newsML.xml\", x[1]), orderedPrestigeVector))\n",
    "\treturn orderedPrestigeVector[:p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePageRank1(D, p, sim, teta, variant='standard', minDocFreq=2, maxDocFreq=0.9, iterations = 10, d = 0.15):\n",
    "\n",
    "\tallTopics = ['R'+str(i) for i in range(101, 200+1)]\n",
    "\t#allTopics = ['R101']\n",
    "\ttopicCount = len(allTopics)\n",
    "\tgraph = build_graph(D, sim, teta, minDocFreq, maxDocFreq)\n",
    "\n",
    "\n",
    "\ttopicIndex=TopicIndex(\"topics.txt\",\"topicsindexdir\")\n",
    "\ttopicQueries=topicIndex.topics\n",
    "\ttrainIndex = WooshDocumentIndex(True, \"trainIndex\", D)\n",
    "\tDids = list(map(lambda x: extractFileId(x)+\"newsML.xml\", D))\n",
    "\n",
    "\tprecisionSum1 = 0\n",
    "\trecallSum1 = 0\n",
    "\tfScoreSum1 = 0\n",
    "\tmapSum1 = 0\n",
    "\tbprefSum1 = 0\n",
    "\n",
    "\tprecisionSum2 = 0\n",
    "\trecallSum2 = 0\n",
    "\tfScoreSum2 = 0\n",
    "\tmapSum2 = 0\n",
    "\tbprefSum2 = 0\n",
    "\n",
    "\tif (variant == 'standard'):\n",
    "\t\tpageRankResult = undirected_page_rank(None, D, p, graph, variant=variant, iterations=iterations)\n",
    "\n",
    "\tfor topic in allTopics:\n",
    "\t\tnormalRetrievalResult = trainIndex.rank(topicQueries[topic.lower()], weighting=scoring.TF_IDF(), k=None).items()\n",
    "\t\tnormalRetrievalResult = sorted(normalRetrievalResult, key= lambda x: x[1], reverse=True)[:p]\n",
    "\n",
    "\t\tprecisionScore, recallScore, fScore, mapScore, bprefScore, recallValues, precisionRecallCurve = calcMetrics(normalRetrievalResult, topic)\n",
    "\t\tsavePrecisionRecallCurve(recallValues, precisionRecallCurve,'report/pagerank/q4/'+topic+'_1.png')\n",
    "\n",
    "\n",
    "\t\tprecisionSum1 += precisionScore\n",
    "\t\trecallSum1 += recallScore\n",
    "\t\tfScoreSum1 += fScore\n",
    "\t\tmapSum1 += mapScore\n",
    "\t\tbprefSum1 += bprefScore\n",
    "\n",
    "\t\tif (variant == 'priors'):\n",
    "\t\t\tpageRankResult = undirected_page_rank(topic, D, p, graph, variant=variant)\n",
    "\t\tprecisionScore, recallScore, fScore, mapScore, bprefScore, recallValues, precisionRecallCurve = calcMetrics(pageRankResult, topic)\n",
    "\t\tsavePrecisionRecallCurve(recallValues,precisionRecallCurve,'report/pagerank/q4/'+topic+'_2.png')\n",
    "\n",
    "\n",
    "\n",
    "\t\tprecisionSum2 += precisionScore\n",
    "\t\trecallSum2 += recallScore\n",
    "\t\tfScoreSum2 += fScore\n",
    "\t\tmapSum2 += mapScore\n",
    "\t\tbprefSum2 += bprefScore\n",
    "\t\t\n",
    "\t\n",
    "\tprint(\"Normal\")\n",
    "\tprint(\"Average Precision: \", precisionSum1/topicCount)\n",
    "\tprint(\"Average Recall: \", recallSum1/topicCount)\n",
    "\tprint(\"Average FScore: \", fScoreSum1/topicCount)\n",
    "\tprint(\"Average MAP: \", mapSum1/topicCount)\n",
    "\tprint(\"Average BPREF: \", bprefSum1/topicCount)\n",
    "\n",
    "\tprint(\"With Page Ranking\")\n",
    "\tprint(\"Average Precision: \", precisionSum2/topicCount)\n",
    "\tprint(\"Average Recall: \", recallSum2/topicCount)\n",
    "\tprint(\"Average FScore: \", fScoreSum2/topicCount)\n",
    "\tprint(\"Average MAP: \", mapSum2/topicCount)\n",
    "\tprint(\"Average BPREF: \", bprefSum2/topicCount)\n",
    "\n",
    "\n",
    "\n",
    "def calcMetrics(result, topic):\n",
    "\tresultDocs = [i[0] for i in result]\n",
    "\tevaledDocs = getEvaledDocsForTopic('qrels.train', topic, 'train')\n",
    "\trelevance = getRelevanceForTopic('qrels.train', topic)\n",
    "\trelevant = [evaledDocs[i] for i in range(len(evaledDocs)) if relevance[i] == 1]\n",
    "\tnonRelevant = [evaledDocs[i] for i in range(len(evaledDocs)) if relevance[i] == 0]\n",
    "\n",
    "\trelevant = list(map(lambda x: extractFileId(x) + \"newsML.xml\", relevant))\n",
    "\tnonRelevant = list(map(lambda x: extractFileId(x) + \"newsML.xml\", nonRelevant))\n",
    "\n",
    "\tprecision = calcPrecision(resultDocs, relevant, nonRelevant)\n",
    "\trecall = calcRecall(resultDocs, relevant, nonRelevant)\n",
    "\tfScoreValue = fscore(precision, recall)\n",
    "\tmapValue = MAP(resultDocs, relevant, nonRelevant)\n",
    "\tbpref = BPREF(resultDocs, relevant, nonRelevant)\n",
    "\trecallValues, precisionAtRecall = precisionRecallCurve(resultDocs, relevant, nonRelevant)\n",
    "\treturn precision, recall, fScoreValue, mapValue, bpref, recallValues, precisionAtRecall\n",
    "\n",
    "def evaluatePageRank2(D, p, sim, minDocFreq=2, maxDocFreq=0.9, iterations = 10, d=0.15):\n",
    "\tallTopics = ['R'+str(i) for i in range(101, 200+1)]\n",
    "\t#allTopics = ['R101']\n",
    "\ttopicCount = len(allTopics)\n",
    "\t\n",
    "\n",
    "\ttrainIndex = WooshDocumentIndex(True, \"trainIndex\", D)\n",
    "\ttopicIndex = TopicIndex(\"topics.txt\",\"topicsindexdir\")\n",
    "\n",
    "\tprecisions = []\n",
    "\trecalls = []\n",
    "\tfScores = []\n",
    "\tmaps = []\n",
    "\tbprefs = []\n",
    "\ttetas = []\n",
    "\n",
    "\tteta = 0.1\n",
    "\twhile (round(teta,2) <= 0.95):\n",
    "\t\tgraph = build_graph(D, sim, teta, minDocFreq, maxDocFreq)\n",
    "\t\tprecisionSum = 0\n",
    "\t\trecallSum = 0\n",
    "\t\tfScoreSum = 0\n",
    "\t\tmapSum = 0\n",
    "\t\tbprefSum = 0\n",
    "\n",
    "\t\tfor topic in allTopics:\n",
    "\t\t\tpageRankResult = undirected_page_rank(None, D, p, graph, variant='standard')\n",
    "\t\t\tprecisionScore, recallScore, fScore, mapScore, bprefScore, recallValues, precisionAtRecall = calcMetrics(pageRankResult, topic)\n",
    "\n",
    "\t\t\tprecisionSum += precisionScore\n",
    "\t\t\trecallSum += recallScore\n",
    "\t\t\tfScoreSum += fScore\n",
    "\t\t\tmapSum += mapScore\n",
    "\t\t\tbprefSum += bprefScore\n",
    "\n",
    "\t\taveragePreccision = precisionSum/topicCount\n",
    "\t\taverageRecall = recallSum/topicCount\n",
    "\t\taverageFscore = fScoreSum/topicCount\n",
    "\t\taverageMap = mapSum/topicCount\n",
    "\t\taverageBref = bprefSum/topicCount\n",
    "\n",
    "\t\tprecisions.append(averagePreccision)\n",
    "\t\trecalls.append(averageRecall)\n",
    "\t\tfScores.append(averageFscore)\n",
    "\t\tmaps.append(averageMap)\n",
    "\t\tbprefs.append(averageBref)\n",
    "\t\ttetas.append(round(teta, 2))\n",
    "\n",
    "\t\tprint(\"Teta: \", round(teta, 2))\n",
    "\t\tprint(\"Average Precision: \", averagePreccision)\n",
    "\t\tprint(\"Average Recall: \", averageRecall)\n",
    "\t\tprint(\"Average FScore: \", averageFscore)\n",
    "\t\tprint(\"Average MAP: \", averageMap)\n",
    "\t\tprint(\"Average BPREF: \", averageBref)\n",
    "\t\tprint()\n",
    "\n",
    "\t\tteta += 0.05\n",
    "\n",
    "\tplt.title(\"Variation of measures for different values of teta\")\n",
    "\tplt.plot(tetas, precisions, label = \"precision\")\n",
    "\tplt.plot(tetas, recalls, label = \"recall\")\n",
    "\tplt.plot(tetas, fScores, label = \"FScore\")\n",
    "\tplt.plot(tetas, maps, label = \"MAP\")\n",
    "\tplt.plot(tetas, bprefs, label = \"BPREF\")\n",
    "\n",
    "\tplt.legend()\n",
    "\tplt.xlabel('teta')\n",
    "\tplt.ylabel('measures')\n",
    "\tplt.show()\n",
    "\n",
    "def evaluatePageRank3(D, p, sim, teta, minDocFreq=2, maxDocFreq=0.9, iterations = 10, d = 0.15):\n",
    "\tallTopics = ['R'+str(i) for i in range(101, 200+1)]\n",
    "\ttopicCount = len(allTopics)\n",
    "\tgraph = build_graph(D, sim, teta, minDocFreq, maxDocFreq)\n",
    "\tpageRankResult = undirected_page_rank(None, D, p, graph, variant='standard')\n",
    "\tprint(pageRankResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('51244newsML.xml', 0.000932902736316949), ('59791newsML.xml', 0.0008932862530311554), ('28141newsML.xml', 0.0008035735804473581), ('72449newsML.xml', 0.0007467131671326183), ('81408newsML.xml', 0.0007348834248564657), ('45867newsML.xml', 0.000733799352203615), ('28710newsML.xml', 0.0007191987885623017), ('37602newsML.xml', 0.0007139687529623545), ('48309newsML.xml', 0.0007094601748488141), ('46422newsML.xml', 0.0007081631720121787)]\n"
     ]
    }
   ],
   "source": [
    "D_train = getEvaledDocs('qrels.train')\n",
    "graph = build_graph(D_train, 'tfidf', 0.2)\n",
    "rankedDocs = undirected_page_rank('R130', D_train, 10, graph, variant='standard')\n",
    "print(rankedDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('15776newsML.xml', 0.004812010950659665), ('46458newsML.xml', 0.003973985156417172), ('18235newsML.xml', 0.003464989311777912), ('4974newsML.xml', 0.001891421064515769), ('4063newsML.xml', 0.0018676696349996357), ('47530newsML.xml', 0.0015880578051225337), ('57876newsML.xml', 0.001537283990087255), ('71894newsML.xml', 0.001492581540261339), ('42848newsML.xml', 0.001467237036641215), ('41857newsML.xml', 0.0013837538948919732)]\n"
     ]
    }
   ],
   "source": [
    "rankedDocs = undirected_page_rank('R130', D_train, 10, graph, variant='priors')\n",
    "print(rankedDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from xml_documents_parser import DocumentParser\n",
    "from html_topics_parser import TopicsParser\n",
    "from Preprocess import Preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def parseDoc(doc):\n",
    "        tokenizer=Preprocess()\n",
    "        parser=DocumentParser()\n",
    "        doc = doc.replace('d_', 'D_')\n",
    "        doc = doc.replace('newsml', 'newsML')\n",
    "        parsedDoc = parser.parse(doc)\n",
    "        words = []\n",
    "        if(parsedDoc.get('title') != None):\n",
    "            words += tokenizer.preprocess(parsedDoc['title'])\n",
    "        if(parsedDoc.get('text') != None):\n",
    "            words +=  tokenizer.preprocess(parsedDoc['text'])\n",
    "        if(parsedDoc.get('byline') != None):\n",
    "            words +=  tokenizer.preprocess(parsedDoc['byline'])\n",
    "        if(parsedDoc.get('dateline') != None):\n",
    "            words +=  tokenizer.preprocess(parsedDoc['dateline'])\n",
    "        return words\n",
    "\n",
    "def noPreprocessDocs(docs):\n",
    "    parser=DocumentParser()\n",
    "    docList = []\n",
    "    for doc in docs:\n",
    "        doc = doc.replace('d_', 'D_')\n",
    "        doc = doc.replace('newsml', 'newsML')\n",
    "        parsedDoc = parser.parse(doc)\n",
    "        words = \"\"\n",
    "        if(parsedDoc.get('title') != None):\n",
    "            words += parsedDoc['title']\n",
    "        if(parsedDoc.get('text') != None):\n",
    "            words +=  parsedDoc['text']\n",
    "        if(parsedDoc.get('byline') != None):\n",
    "            words +=  parsedDoc['byline']\n",
    "        if(parsedDoc.get('dateline') != None):\n",
    "            words +=  parsedDoc['dateline']\n",
    "        docList.append(words)\n",
    "    return docList\n",
    "\n",
    "def noPreprocessTopics(path):\n",
    "    parser = TopicsParser()\n",
    "    splittedTopics = parser.get_data(path)\n",
    "    topics = []\n",
    "    for i in splittedTopics:\n",
    "        topicText = ''\n",
    "        topicText += i['title'] + ' '\n",
    "        topicText += i['desc'] + ' '\n",
    "        topicText += i['narr']\n",
    "        topics.append(topicText)\n",
    "    return topics\n",
    "\n",
    "def pointDistance(p1, p2):\n",
    "    return math.sqrt((p1[0]-p2[0])**2 + (p1[0]-p2[0])**2)\n",
    "\n",
    "def getAllFiles(path):\n",
    "    subdirs = listdir(path)\n",
    "    docs = []\n",
    "    for subdir in subdirs:\n",
    "        subdirFiles = listdir(join(path, subdir))\n",
    "        docs += list(map(lambda x : path + \"/\" + subdir+ \"/\" +x, subdirFiles))\n",
    "    return docs\n",
    "\n",
    "def getEvaledDocs(filename):\n",
    "    if (filename == 'qrels.train'):\n",
    "        allFiles = getAllFiles('./rcv1/D_train')\n",
    "    else:\n",
    "        allFiles = getAllFiles('./rcv1/D_test')\n",
    "    f = open(filename, 'r')\n",
    "    relDocsIds = set()\n",
    "    for line in f:\n",
    "        triplet = line.split()\n",
    "        relDocsIds.add(triplet[1])\n",
    "    f.close()\n",
    "    #relDocs = set(filter(lambda x: extractFileId(x) in relDocsIds , allFiles))\n",
    "    relDocs = list(filter(lambda x: extractFileId(x) in relDocsIds , allFiles))\n",
    "\n",
    "    return relDocs\n",
    "\n",
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "\n",
    "def extractFileId(path):\n",
    "    filenameOnlyExp = re.search('/[^/]*?newsML\\.xml', path)\n",
    "    filenameOnly = ''\n",
    "    if (filenameOnlyExp):\n",
    "        filenameOnly = filenameOnlyExp.group()[1:-10]\n",
    "        return filenameOnly\n",
    "\n",
    "def getEvaledDocsForTopic(filename, topic, col): #col 'test' or 'train'\n",
    "    if(col == 'test'):\n",
    "        allFiles = getAllFiles('./rcv1/D_test')\n",
    "    if(col == 'train'):\n",
    "        allFiles = getAllFiles('./rcv1/D_train')\n",
    "    \n",
    "    f = open(filename, 'r')\n",
    "    relDocsIds = set()\n",
    "    for line in f:\n",
    "        triplet = line.split()\n",
    "        if(triplet[0] == topic):\n",
    "            relDocsIds.add(triplet[1])\n",
    "    f.close()\n",
    "\n",
    "    relDocs = list(filter(lambda x: extractFileId(x) in relDocsIds , allFiles))\n",
    "    return relDocs\n",
    "\n",
    "\n",
    "\n",
    "def getRelevanceForTopic(filename, topic):\n",
    "    f = open(filename, 'r')\n",
    "    relevance = []\n",
    "    for line in f:\n",
    "        triplet = line.split()\n",
    "        if(triplet[0] == topic):\n",
    "            relevance.append(int(triplet[2]))\n",
    "    return relevance\n",
    "\n",
    "def VectorizerParseDoc(doc):\n",
    "        tokenizer=Preprocess()\n",
    "        parser=DocumentParser()\n",
    "        parsedDoc = parser.parse(doc.replace('d_train', 'D_train').replace('d_test', 'D_test').replace('newsml.xml', 'newsML.xml'))\n",
    "        words = []\n",
    "        if(parsedDoc.get('title') != None):\n",
    "            words += tokenizer.preprocess(parsedDoc['title'])\n",
    "        if(parsedDoc.get('text') != None):\n",
    "            words +=  tokenizer.preprocess(parsedDoc['text'])\n",
    "        if(parsedDoc.get('byline') != None):\n",
    "            words +=  tokenizer.preprocess(parsedDoc['byline'])\n",
    "        if(parsedDoc.get('dateline') != None):\n",
    "            words +=  tokenizer.preprocess(parsedDoc['dateline'])\n",
    "        return words\n",
    "    \n",
    "def getDocsForTopics(filename, topics, allFiles): #col 'test' or 'train'    \n",
    "    f = open(filename, 'r')\n",
    "    DTrain=dict()\n",
    "    RTrain=dict()\n",
    "    docs_ids=set()\n",
    "    for line in f:\n",
    "        triplet = line.split()\n",
    "        if(triplet[0] in topics):\n",
    "            docs_ids.add(triplet[1])\n",
    "            try:\n",
    "                DTrain[triplet[0]].append(triplet[1])\n",
    "                RTrain[triplet[0]].append(int(triplet[2]))\n",
    "            except KeyError:\n",
    "                DTrain[triplet[0]]=[triplet[1]]\n",
    "                RTrain[triplet[0]]=[int(triplet[2])]        \n",
    "    f.close()\n",
    "    return DTrain,RTrain,docs_ids\n",
    "\n",
    "def getCollectionVector(topics):\n",
    "    #'Train'\n",
    "    TrainFiles = getAllFiles('./rcv1/D_train')\n",
    "    DTrain,RTrain,train_docs_ids=getDocsForTopics('qrels.train',topics,TrainFiles)\n",
    "    #'Test'\n",
    "    TestFiles = getAllFiles('./rcv1/D_test')\n",
    "    DTest,RTest,test_docs_ids=getDocsForTopics('qrels.test',topics,TestFiles)\n",
    "    fileRepresentation=dict()\n",
    "    \n",
    "    allFiles=TrainFiles+TestFiles\n",
    "    docs_ids=train_docs_ids.union(test_docs_ids)\n",
    "    allFiles=[file for file in allFiles if extractFileId(file) in docs_ids]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(tokenizer=VectorizerParseDoc,use_idf = False)\n",
    "    vectorspace = vectorizer.fit_transform(allFiles)\n",
    "    for i in range(len(allFiles)):\n",
    "        fileRepresentation[extractFileId(allFiles[i])]=vectorspace[i].toarray()[0]\n",
    "        #print(type(vectorspace[i]),np.shape(vectorspace[i]),' ',np.shape(vectorspace[i].toarray()[0]))     \n",
    "        #fileRepresentation[file_id]=vectorspace.toarray()[0]\n",
    "    for topic in topics:\n",
    "        for i in range(len(DTrain[topic])):\n",
    "            DTrain[topic][i]=fileRepresentation[DTrain[topic][i]]\n",
    "        for i in range(len(DTest[topic])):\n",
    "            DTest[topic][i]=fileRepresentation[DTest[topic][i]]\n",
    "    return DTrain,RTrain,DTest,RTest\n",
    "\n",
    "def getCollection(topics):\n",
    "    #'Train'\n",
    "    TrainFiles = getAllFiles('./rcv1/D_train')\n",
    "    DTrain,RTrain,train_docs_ids=getDocsForTopics('qrels.train',topics,TrainFiles)\n",
    "    #'Test'\n",
    "    TestFiles = getAllFiles('./rcv1/D_test')\n",
    "    DTest,RTest,test_docs_ids=getDocsForTopics('qrels.test',topics,TestFiles)    \n",
    "    \n",
    "    #files=TrainFiles+TestFiles\n",
    "    #docs_ids=train_docs_ids.union(test_docs_ids)\n",
    "    TrainFiles=[file for file in TrainFiles if extractFileId(file) in train_docs_ids]\n",
    "    TestFiles=[file for file in TestFiles if extractFileId(file) in test_docs_ids]\n",
    "    return TrainFiles,TestFiles,DTrain,RTrain,DTest,RTest\n",
    "\n",
    "\n",
    "def computeSum(vector):\n",
    "    sum1 = 0\n",
    "    for k,v in vector.items():\n",
    "        sum1 += v\n",
    "    return sum1\n",
    "\n",
    "def fillVector(allEntries, vector):\n",
    "    for entry in allEntries:\n",
    "        if (vector.get(entry) == None):\n",
    "            vector[entry] = 0\n",
    "\n",
    "def normalizeDict(d):\n",
    "    vecSum = computeSum(d)\n",
    "    for k,v in d.items():\n",
    "        d[k] = v/vecSum\n",
    "\n",
    "        \n",
    "def getRelevantNonRelevant(topic):\n",
    "    evaledDocs = getEvaledDocsForTopic('qrels.test', topic, 'test')\n",
    "    relevance = getRelevanceForTopic('qrels.test', topic)\n",
    "    relevant = [extractFileId(evaledDocs[i]) for i in range(len(evaledDocs)) if relevance[i] == 1]\n",
    "    nonRelevant = [extractFileId(evaledDocs[i]) for i in range(len(evaledDocs)) if relevance[i] == 0]\n",
    "    return relevant,nonRelevant\n",
    "\n",
    "def setupEvalManyFeatures(search_topics, DTrain, DTest,trainDocsIndex,testDocsIndex,topicsIndex):\n",
    "    topics=topicsIndex.topics\n",
    "\n",
    "    train_bm25=dict()\n",
    "    train_cos=dict()\n",
    "    train_freq=dict()\n",
    "    test_bm25=dict()\n",
    "    test_cos=dict()\n",
    "    test_freq=dict()\n",
    "\n",
    "    for topic in search_topics:\n",
    "        train_bm25[topic],train_cos[topic],train_freq[topic] =trainDocsIndex.generate_scores(topics[topic.lower()], k=None)\n",
    "        test_bm25[topic],test_cos[topic],test_freq[topic]=testDocsIndex.generate_scores(topics[topic.lower()], k=None)\n",
    "\n",
    "    trainX=dict()\n",
    "    testX=dict()\n",
    "    \n",
    "    for topic in search_topics:\n",
    "        trainX[topic]=list()\n",
    "        for fileID in DTrain[topic.upper()]:\n",
    "            fileName=fileID+\"newsML.xml\"\n",
    "            try :\n",
    "                value=[train_bm25[topic][fileName],train_cos[topic][fileName],\n",
    "                                     train_freq[topic][fileName]]\n",
    "                trainX[topic].append(value)\n",
    "            except:\n",
    "                trainX[topic].append([0,0,0])\n",
    "                \n",
    "        testX[topic]=list()\n",
    "        for fileID in DTest[topic.upper()]:\n",
    "            fileName=fileID+\"newsML.xml\"\n",
    "            try :\n",
    "                value=[test_bm25[topic][fileName],test_cos[topic][fileName],\n",
    "                                    test_freq[topic][fileName]]\n",
    "                testX[topic].append(value)\n",
    "            except:\n",
    "                testX[topic].append([0,0,0])\n",
    "    return trainX, testX\n",
    "\n",
    "def setupEvalOneFeature(search_topics, DTrain, DTest,trainDocsIndex,testDocsIndex,topicsIndex):\n",
    "    topics=topicsIndex.topics\n",
    "\n",
    "    train_bm25=dict()\n",
    "    test_bm25=dict()\n",
    "\n",
    "    for topic in search_topics:\n",
    "        train_bm25[topic] =trainDocsIndex.generate_score(topics[topic.lower()],measure='bm25', k=None)\n",
    "        test_bm25[topic]=testDocsIndex.generate_score(topics[topic.lower()],measure='bm25',k=None)\n",
    "\n",
    "    trainX=dict()\n",
    "    testX=dict()\n",
    "    \n",
    "    for topic in search_topics:\n",
    "        trainX[topic]=list()\n",
    "        for fileID in DTrain[topic.upper()]:\n",
    "            fileName=fileID+\"newsML.xml\"\n",
    "            try :\n",
    "                value=[train_bm25[topic][fileName]]\n",
    "                trainX[topic].append(value)\n",
    "            except:\n",
    "                trainX[topic].append([0])\n",
    "                \n",
    "        testX[topic]=list()\n",
    "        for fileID in DTest[topic.upper()]:\n",
    "            fileName=fileID+\"newsML.xml\"\n",
    "            try :\n",
    "                value=[test_bm25[topic][fileName]]\n",
    "                testX[topic].append(value)\n",
    "            except:\n",
    "                testX[topic].append([0])\n",
    "    return trainX, testX\n",
    "\n",
    "def RRF(rankingsList):\n",
    "    rrf = 0\n",
    "    #for score in rankingsList:\n",
    "    #        rrf += (1 / (50 + score))\n",
    "    i=3\n",
    "    for score in rankingsList:\n",
    "        rrf+=i*score\n",
    "        i-=1\n",
    "    return rrf\n",
    "\n",
    "def fscoreFunc(precision, recall):\n",
    "    fscore=0\n",
    "    if(precision == 0 or recall == 0):\n",
    "        return fscore\n",
    "    else:\n",
    "        fscore= 2 / ( (1/recall) + (1/precision))\n",
    "        return fscore\n",
    "    \n",
    "def BPREF(queryResult, relevant, nonRelevant):\n",
    "    _sum = 0\n",
    "    relevant=set(relevant)\n",
    "    nonRelevant=set(nonRelevant)\n",
    "    relevantSize = len(relevant)\n",
    "    nonRelevantSize = len(nonRelevant.intersection(set(queryResult)))\n",
    "    dividingFactor = min(relevantSize,nonRelevantSize)\n",
    "\n",
    "    judgedAndRetrieved = set(queryResult).intersection(relevant)\n",
    "    judgedAndRetrievedSize = len(judgedAndRetrieved)\n",
    "    if(nonRelevantSize == 0):\n",
    "        return 1\n",
    "    for i in range(1,judgedAndRetrievedSize+1):\n",
    "        judgedAndRelevantAndAk = (nonRelevant).intersection(set(queryResult[:i]))\n",
    "        _sum += float(1 - (len(judgedAndRelevantAndAk)) / dividingFactor)\n",
    "    return float(float(_sum) / relevantSize)\n",
    "\n",
    "def calcPrecision(retrieved, relevant, nonRelevant):\n",
    "    truePositives = len(set(retrieved).intersection(relevant))\n",
    "    falsePositives = len(set(retrieved).intersection(nonRelevant)) \n",
    "    if( truePositives + falsePositives == 0):\n",
    "        return 0\n",
    "    return truePositives / (truePositives + falsePositives)\n",
    "\n",
    "def calcRecall(retrieved,relevant,nonRelevant):\n",
    "    truePositives = len(set(retrieved).intersection(relevant))\n",
    "    falseNegatives=len((set(relevant)|set(nonRelevant)-set(retrieved)).intersection(relevant))\n",
    "    if( truePositives + falseNegatives == 0):\n",
    "        return 0\n",
    "    return truePositives / (truePositives + falseNegatives)\n",
    "\n",
    "def MAP(retrieved, relevantFiles, nonRelevantFiles):\n",
    "    precisionSum = 0\n",
    "    k = len(retrieved)\n",
    "    if(k == 0):\n",
    "        return 0\n",
    "    for i in range(1, k+1):\n",
    "        if (retrieved[i-1] in relevantFiles):\n",
    "            precisionSum += float(calcPrecision(retrieved[:i], relevantFiles, nonRelevantFiles))\n",
    "    return float(precisionSum) / float(k)\n",
    "\n",
    "def fscore(precision, recall):\n",
    "    if(precision == 0 or recall == 0):\n",
    "        return 0\n",
    "    return 2 / ( (1/recall) + (1/precision))\n",
    "\n",
    "def averagePrecision(precisionRecallCurve):\n",
    "    total=0\n",
    "    count=0\n",
    "    #get the precision values from the precision recall curve\n",
    "    for precision in precisionRecallCurve[1][1:]:\n",
    "        total+=precision\n",
    "        count+=1\n",
    "    return (precision/count)\n",
    "\n",
    "def calcPrecisionAtRecall(recall,retrieved,relevant,nonRelevant):\n",
    "    if (recall==0):\n",
    "        return 1;\n",
    "    return calcPrecision(retrieved[0:recall],relevant,nonRelevant)\n",
    "\n",
    "def precisionRecallCurve(query_result,relevantFiles,nonRelevantFiles):\n",
    "    if not query_result:\n",
    "        return [],[]\n",
    "    recallValues=[]\n",
    "    precisionAtRecall = []\n",
    "\n",
    "    for recall_value in range(len(query_result)+1):#0.1 intervals\n",
    "        result=calcPrecisionAtRecall(recall_value,query_result,relevantFiles,nonRelevantFiles)\n",
    "        precisionAtRecall.append(result)\n",
    "        if(len(query_result) == 0):\n",
    "            recallValues.append(0)\n",
    "            continue\n",
    "        recallValues.append(recall_value/len(query_result))\n",
    "    return recallValues,precisionAtRecall\n",
    "\n",
    "def plotPrecisionRecallCurve(recall,precision):\n",
    "    if not precision:\n",
    "        print(\"Weren't retrieved any documents with this type of ranking!\")\n",
    "        return None\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(recall)-1):\n",
    "        ax.plot((recall[i],recall[i]),(precision[i],precision[i+1]),'k-',color='red') #vertical\\n\",\n",
    "        ax.plot((recall[i],recall[i+1]),(precision[i+1],precision[i+1]),'k-',color='red') #horizontal\\n\"\n",
    "    ax.set_xlabel(\"recall\")\n",
    "    ax.set_ylabel(\"precision\")\n",
    "    plt.axis([0.0,1.1,0.0,1.1])\n",
    "    plt.show()\n",
    "\n",
    "def savePrecisionRecallCurve(recall,precision,path):\n",
    "    if not precision:\n",
    "        print(\"Weren't retrieved any documents with this type of ranking!\")\n",
    "        return None\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(recall)-1):\n",
    "        ax.plot((recall[i],recall[i]),(precision[i],precision[i+1]),'k-',color='red') #vertical\\n\",\n",
    "        ax.plot((recall[i],recall[i+1]),(precision[i+1],precision[i+1]),'k-',color='red') #horizontal\\n\"\n",
    "    ax.set_xlabel(\"recall\")\n",
    "    ax.set_ylabel(\"precision\")\n",
    "    plt.axis([0.0,1.1,0.0,1.1])\n",
    "    plt.savefig(path)\n",
    "\n",
    "def compute_metrics(docs_names, relevant, nonRelevant,k):\n",
    "    precision = calcPrecision(docs_names[:k], relevant, nonRelevant)\n",
    "    recall = calcRecall(docs_names[:k], relevant, nonRelevant)\n",
    "    fscoreVal = fscoreFunc(precision,recall)\n",
    "    precision_recall_curve=precisionRecallCurve(docs_names[:k],relevant,nonRelevant)\n",
    "    bpref=BPREF(docs_names[:k], relevant, nonRelevant)\n",
    "    #avg_precision=averagePrecision(precision_recall_curve)\n",
    "    _MAP=MAP(docs_names[:k],relevant,nonRelevant)\n",
    "    return precision,recall,fscoreVal,precision_recall_curve,bpref,_MAP\n",
    "    \n",
    "def show_graphics_metrics_IRmodels(IRmodels,IRmodels_names,search_topics):\n",
    "    #print(IRmodels)\n",
    "    for topic in search_topics:\n",
    "        for i in range(len(IRmodels)):\n",
    "                print(IRmodels_names[i],\" TOPIC \",topic)\n",
    "                plotPrecisionRecallCurve(IRmodels[i][topic][3][0],IRmodels[i][topic][3][1])\n",
    "    \n",
    "    print(\"----------IR Models Summary----------\\n\")\n",
    "    for i in range(len(IRmodels)): \n",
    "        print(IRmodels_names[i],'\\n')\n",
    "        print('Avg MAP:',IRmodels[i]['Avg MAP'], ' Avg BPREF:',IRmodels[i]['Avg BPREF'],'\\n')\n",
    "    print('------------------------------------')\n",
    "        \n",
    "def show_metrics_classifiers(classifiers,classifiers_names,search_topics): \n",
    "    for topic in search_topics:\n",
    "        for i in range(len(classifiers)):\n",
    "                print(classifiers_names[i],\" TOPIC \",topic)\n",
    "                precision,recall,fscore,avg_prec_score=classifiers[i][topic]\n",
    "                print(\"Precision:\",precision,\" Recall:\",recall,\" F-Score\",fscore,\" MAP:\",avg_prec_score,'\\n')\n",
    "    \n",
    "    print(\"---------- Classifiers Summary----------\\n\")\n",
    "    for i in range(len(classifiers)): \n",
    "        print(classifiers_names[i],' ','Avg MAP:',classifiers[i]['Avg MAP'],'\\n')\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from autocorrect import spell\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "class Preprocess:\n",
    "    def __int__(self):\n",
    "        pass\n",
    "\n",
    "    def autospell(self,text):\n",
    "        spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "        return \" \".join(spells)\n",
    "\n",
    "    def to_lower(self,text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_numbers(self,text):\n",
    "        output = ''.join(c for c in text if not c.isdigit())\n",
    "        return output\n",
    "\n",
    "    def remove_punct(self,text):\n",
    "        return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "    def remove_Tags(self,text):\n",
    "        cleaned_text = re.sub('<[^<]+?>', '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def sentence_tokenize(self,text):\n",
    "        sent_list = []\n",
    "        for w in nltk.sent_tokenize(text):\n",
    "            sent_list.append(w)\n",
    "        return sent_list\n",
    "\n",
    "    def word_tokenize(self,text):\n",
    "        return [w for sent in nltk.sent_tokenize(text) for w in nltk.word_tokenize(sent)]\n",
    "\n",
    "    def remove_stopwords(self,sentence):\n",
    "        stop_words = stopwords.words('english')\n",
    "        return ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words])\n",
    "\n",
    "    def stem(self,text):\n",
    "        stemmed_word = [snowball_stemmer.stem(word) for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "        return \" \".join(stemmed_word)\n",
    "\n",
    "    def lemmatize(self,text):\n",
    "        lemmatized_word = [wordnet_lemmatizer.lemmatize(word)for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "        return \" \".join(lemmatized_word)\n",
    "\n",
    "\n",
    "    def preprocess(self,text):\n",
    "        lower_text = self.to_lower(text)\n",
    "        sentence_tokens = self.sentence_tokenize(lower_text)\n",
    "        word_list = []\n",
    "        for each_sent in sentence_tokens:\n",
    "            lemmatizzed_sent = self.lemmatize(each_sent)\n",
    "            clean_text = self.remove_numbers(lemmatizzed_sent)\n",
    "            clean_text = self.remove_punct(clean_text)\n",
    "            clean_text = self.remove_Tags(clean_text)\n",
    "            clean_text = self.remove_stopwords(clean_text)\n",
    "            word_tokens = self.word_tokenize(clean_text)\n",
    "            for i in word_tokens:\n",
    "                word_list.append(i)\n",
    "        return word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class TopicsParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.last_tag = None\n",
    "        self.topics_list=[]\n",
    "        self.dictionary={}\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.last_tag=tag\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        self.last_tag='top'\n",
    "        self.topics_list.append(self.dictionary.copy())\n",
    "        self.dictionary.clear()\n",
    "        \n",
    "    def handle_data(self, data):\n",
    "        if self.last_tag!='top':\n",
    "            if self.last_tag!='title':\n",
    "                _index=data.find(':')\n",
    "                if _index!=-1:\n",
    "                    data=data[_index+1:].strip()\n",
    "            \n",
    "            self.dictionary[self.last_tag]=data\n",
    "    \n",
    "    def get_data(self,file_path):\n",
    "        file=open(file_path,'r')\n",
    "        self.feed(file.read().lower())\n",
    "        return self.topics_list.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class DocumentParser():\n",
    "    def __init__(self):\n",
    "        self.tags=['title','headline','byline','dateline','text']\n",
    "        self.dictionary={}\n",
    "        \n",
    "    def parse(self,path):\n",
    "        self.dictionary.clear()\n",
    "        try:\n",
    "            self.root = ET.parse(path).getroot()\n",
    "        except:\n",
    "            print(\"Wrong file path when parsing document!\")\n",
    "        for child in self.root:\n",
    "            if child.tag in self.tags:\n",
    "                if child.tag!='text':\n",
    "                    self.dictionary[child.tag]=child.text\n",
    "                if child.tag=='text':\n",
    "                    self.dictionary['text']=''\n",
    "                    for p in child:\n",
    "                        self.dictionary['text'] += ' '+p.text\n",
    "        return self.dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whoosh Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml_documents_parser import DocumentParser\n",
    "from html_topics_parser import TopicsParser\n",
    "from Preprocess import Preprocess\n",
    "import nltk\n",
    "import numpy as np\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema,TEXT\n",
    "from whoosh import scoring\n",
    "from whoosh.qparser import QueryParser, OrGroup\n",
    "from whoosh.index import open_dir\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TopicIndex():\n",
    "    def __init__(self,topics_filename,dir_name):\n",
    "        self.parser=TopicsParser()\n",
    "        self.preProcess=Preprocess()\n",
    "        self.preprocessed=True\n",
    "        self.topics_parsed = self.parser.get_data(topics_filename)\n",
    "        self.topics=dict()\n",
    "        for topic in self.topics_parsed:\n",
    "            self.topics[topic['num']]=\" \".join(self.preProcess.preprocess(topic['title']\n",
    "                +' '+topic['narr']+' '+topic['desc']))\n",
    "\n",
    "class WooshDocumentIndex():\n",
    "   \n",
    "    def __init__(self,load,dir_name,files):\n",
    "        self.preProcess=Preprocess()\n",
    "        self.documentParser=DocumentParser()\n",
    "        self.preprocessed=True\n",
    "        if not load:\n",
    "            if not os.path.isdir(dir_name):\n",
    "                os.mkdir(dir_name)\n",
    "            schema = Schema(id = TEXT(stored=True), content=TEXT(stored=True))\n",
    "            self.ix = create_in(dir_name, schema)\n",
    "            self.index(files)    \n",
    "        else:\n",
    "            self.ix=open_dir(dir_name)\n",
    "                    \n",
    "    def index(self,files):\n",
    "        writer = self.ix.writer()\n",
    "        #Read file.\n",
    "        i=0\n",
    "        fl = len(files)\n",
    "        for fname in tqdm(range(fl)):\n",
    "            parsedDoc = self.documentParser.parse(files[fname])\n",
    "            words = []\n",
    "\n",
    "            if(self.preprocessed):\n",
    "                if(parsedDoc.get('title') != None):\n",
    "                    words += self.preProcess.preprocess(parsedDoc['title'])\n",
    "                if(parsedDoc.get('text') != None):\n",
    "                    words +=  self.preProcess.preprocess(parsedDoc['text'])\n",
    "                if(parsedDoc.get('byline') != None):\n",
    "                    words +=  self.preProcess.preprocess(parsedDoc['byline'])\n",
    "                if(parsedDoc.get('dateline') != None):\n",
    "                    words +=  self.preProcess.preprocess(parsedDoc['dateline'])\n",
    "            else:\n",
    "                if(parsedDoc.get('title') != None):\n",
    "                    words += nltk.word_tokenize(parsedDoc['title'].lower())\n",
    "                if(parsedDoc.get('text') != None):\n",
    "                    words +=  nltk.word_tokenize(parsedDoc['text'].lower())\n",
    "                if(parsedDoc.get('byline') != None):\n",
    "                    words +=  nltk.word_tokenize(parsedDoc['byline'].lower())\n",
    "                if(parsedDoc.get('dateline') != None):\n",
    "                    words +=  nltk.word_tokenize(parsedDoc['dateline'].lower())\n",
    "                    \n",
    "            filenameOnlyExp = re.search('/[^/]*?newsML\\.xml', files[fname])\n",
    "            if (filenameOnlyExp):\n",
    "                filenameOnlyExp = filenameOnlyExp.group()[1:]\n",
    "            writer.add_document(id=filenameOnlyExp,content=words)\n",
    "        writer.commit()\n",
    "\n",
    "    def rank(self,query_str,weighting=scoring.BM25F(),k=None):\n",
    "        ''' Perform a query using the weighting scoring function and obtain the corresponding textual similarity score. '''\n",
    "        ix = self.ix\n",
    "    \n",
    "        with ix.searcher(weighting=weighting) as searcher:\n",
    "            query = QueryParser(\"content\", ix.schema, group=OrGroup).parse(query_str)\n",
    "            results = searcher.search(query,limit=k)#,scored=None,sortedby=None)\n",
    "            query_res = dict()\n",
    "            for i,r in enumerate(results):\n",
    "                id = r['id']\n",
    "                #print(i,results.score(i),r['id'],r['content'],'\\n')\n",
    "                query_res[id] = results.score(i)\n",
    "            return query_res\n",
    "        \n",
    "    def generate_scores(self,query,k=None):\n",
    "        '''Generate scores for a given query according to BM25, TF IDF (under a cosine similarity) and Frequency rank functions'''\n",
    "        bm25 = self.rank(query,weighting=scoring.BM25F(),k=k)\n",
    "        cos = self.rank(query,weighting=scoring.TF_IDF(),k=k)\n",
    "        freq = self.rank(query,weighting=scoring.Frequency(),k=k)\n",
    "        return bm25,cos,freq \n",
    "    \n",
    "    def generate_score(self,query,measure,k=None):\n",
    "        '''Generate scores for a given query according to a given measure'''\n",
    "        if(measure=='bm25'):\n",
    "            score = self.rank(query,weighting=scoring.BM25F(),k=k)\n",
    "        elif(measure=='cos'):\n",
    "            score = self.rank(query,weighting=scoring.TF_IDF(),k=k)\n",
    "        elif(measure=='freq'):\n",
    "            score = self.rank(query,weighting=scoring.Frequency(),k=k)\n",
    "        return score \n",
    "    \n",
    "    def score(self,n_docs,n_rel,bm25,cos,freq,pagerank,alpha_1=3,alpha_2=3,alpha_3=1,alpha_4=1):\n",
    "        scores = dict()\n",
    "        #Iterate over all documents in collection.\n",
    "        for k,v in bm25.items():\n",
    "            #Rank combination.\n",
    "            scores[k] = alpha_1*bm25[k] + alpha_2*cos[k] + alpha_3*freq[k] #+ alpha_4 * pagerank[k]\n",
    "            if(pagerank.get(k) != None):\n",
    "                scores[k] += alpha_4 * pagerank[k]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.metrics import precision_recall_fscore_support,average_precision_score\n",
    "from util import compute_metrics, RRF\n",
    "\n",
    "class LogisticClassifier():\n",
    "    def __init__(self,**args):\n",
    "        hyper_parameters=args.get('hyper_parameters')\n",
    "        if(hyper_parameters!=None):\n",
    "            solver=hyper_parameters['solver']\n",
    "            C=hyper_parameters['C']\n",
    "            penalty=hyper_parameters['penalty']\n",
    "            self.clf = LogisticRegression(\n",
    "                    random_state=1, solver=solver,C=C,penalty=penalty,multi_class='multinomial')\n",
    "        else:\n",
    "            self.clf=LogisticRegression(random_state=1,solver='lbfgs',multi_class='multinomial')\n",
    "        \n",
    "    def train(self,topic,DTrain,RTrain,**kwargs):\n",
    "        DTrain=DTrain[topic]\n",
    "        RTrain=RTrain[topic]\n",
    "        self.clf.fit(DTrain,RTrain)\n",
    "        \n",
    "    \n",
    "    def classify(self,doc,topic,**kwargs):\n",
    "        return (self.clf.predict_proba([doc])[0])\n",
    "                    \n",
    "    def evaluate(self,topic,DTest,RTest,**kwargs):\n",
    "        k = kwargs.get('k')\n",
    "        testX=kwargs.get('testX')\n",
    "        ranking_type=kwargs.get('ranking_type')\n",
    "        relevant=kwargs.get('relevant')\n",
    "        nonRelevant=kwargs.get('nonRelevant')\n",
    "        '''\n",
    "        Evaluating the classifier\n",
    "        '''\n",
    "        docs=testX[topic]\n",
    "        feedback=RTest[topic]\n",
    "        y_scores=self.clf.predict_proba(docs)\n",
    "        y_pred=self.clf.predict(docs)\n",
    "        precision,recall,fscore,true_sum = precision_recall_fscore_support(feedback, y_pred\n",
    "                                                , average='macro',zero_division=1)\n",
    "        avg_prec_score = average_precision_score(feedback,y_scores[:,1])\n",
    "        classifier_metrics=[precision,recall,fscore,avg_prec_score]\n",
    "        '''\n",
    "        Binary retrieval\n",
    "        '''\n",
    "        scores_names=zip(y_scores,DTest[topic]) \n",
    "        positive_class_predicted=[doc for doc in scores_names if doc[0][1]>0.5]\n",
    "        aided_non_ranked_docs_names=[doc[1] for doc in positive_class_predicted]\n",
    "        '''\n",
    "        Evaluating binary retrieval\n",
    "        '''\n",
    "        precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec=compute_metrics(aided_non_ranked_docs_names\n",
    "                                                                          ,relevant, nonRelevant,k)\n",
    "        aidedNonRanked=[precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "        '''\n",
    "        Extension towards ranking\n",
    "        '''\n",
    "        if ranking_type=='proba':#sorts according to probabilities\n",
    "            aided_ranked_docs_names=[x for _, x in sorted(zip(y_scores,DTest[topic]), \n",
    "                                                    key=lambda pair: pair[0][1],reverse=True)]\n",
    "        else:#sort according to the score of the docs classified as positive\n",
    "            aided_ranked_docs_names=[doc[1] for doc in sorted(positive_class_predicted,\n",
    "                                           key=lambda x:RRF(x[0]),reverse=True)[:k]]\n",
    "        '''\n",
    "        Evaluating Aided IR\n",
    "        '''\n",
    "        precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec=compute_metrics(aided_ranked_docs_names\n",
    "                                                                          ,relevant, nonRelevant,k)\n",
    "        aidedRanked=[precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "        return aidedRanked,aidedNonRanked,classifier_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support,average_precision_score\n",
    "from statistics import mean\n",
    "from util import getRelevantNonRelevant\n",
    "from util import compute_metrics, RRF\n",
    "\n",
    "class XGBOOSTClassifier():\n",
    "    def __init__(self):\n",
    "        self.clf = XGBClassifier(n_estimators=100,max_depth=100)\n",
    "        \n",
    "    def train(self,topic,DTrain,RTrain,**kwargs):\n",
    "        DTrain=DTrain[topic]\n",
    "        RTrain=RTrain[topic]\n",
    "        DTrain = StandardScaler().fit_transform(DTrain)\n",
    "        self.clf.fit(DTrain,RTrain)\n",
    "        \n",
    "    \n",
    "    def classify(self,doc,topic,**kwargs):\n",
    "        return self.clf.predict_proba([doc])[0]\n",
    "                    \n",
    "    def evaluate(self,topics,DTest,RTest,**kwargs):\n",
    "        avg_prec_scores = []\n",
    "        RTrain=kwargs.get('RTrain')\n",
    "        k = kwargs.get('k')\n",
    "        trainX=kwargs.get('trainX')\n",
    "        testX=kwargs.get('testX')\n",
    "        ranking_type=kwargs.get('ranking_type')\n",
    "        '''\n",
    "        Metricss Data structures\n",
    "        '''\n",
    "        aidedRanked=dict()\n",
    "        aidedNonRanked=dict()\n",
    "        nonAidedIROutput = dict()\n",
    "        classifier_metrics=dict()\n",
    "        '''\n",
    "        Evaluation for each topic\n",
    "        '''\n",
    "        for topic in topics:\n",
    "            relevant,nonRelevant=getRelevantNonRelevant(topic)\n",
    "            '''\n",
    "            Non-Aided IR\n",
    "            '''\n",
    "            ranked_docs_names=[name for score, name in sorted(zip(testX[topic],DTest[topic]), \n",
    "                                key=lambda pair: RRF(pair[0]),reverse=True)]\n",
    "            precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec=compute_metrics(ranked_docs_names\n",
    "                                                                              ,relevant, nonRelevant,k)\n",
    "            nonAidedIROutput[topic] = [precision, recall, fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "            '''\n",
    "            Trainning the classifier\n",
    "            '''\n",
    "            try:\n",
    "                self.train(topic,trainX,RTrain)\n",
    "            except ValueError:\n",
    "                print(\"For topic \", topic\n",
    "                      ,\"the classifier needs samples of at least 2 classes in the data\"\n",
    "                      , \"but the data contains only one class: 1\")\n",
    "                #the behaviour with the classifier would not change since there is no information in the data\n",
    "                aidedRanked[topic]=[precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "                aidedNonRanked[topic]=[precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "                avg_prec_scores += [0.5]#no information in the classifier\n",
    "                continue\n",
    "            '''\n",
    "            Evaluating the classifier\n",
    "            '''\n",
    "            docs=testX[topic]\n",
    "            docs = StandardScaler().fit_transform(docs)\n",
    "            feedback=RTest[topic]\n",
    "            y_scores=self.clf.predict_proba(docs)\n",
    "            y_pred=self.clf.predict(docs)\n",
    "            precision,recall,fscore,true_sum = precision_recall_fscore_support(feedback, y_pred\n",
    "                                                    , average='macro',zero_division=1)\n",
    "            avg_prec_scores += [average_precision_score(feedback,y_scores[:,1])]\n",
    "            classifier_metrics[topic]=[precision,recall,fscore]\n",
    "            '''\n",
    "            Binary retrieval\n",
    "            '''\n",
    "            scores_names=zip(y_scores,DTest[topic]) \n",
    "            positive_class_predicted=[doc for doc in scores_names if doc[0][1]>0.5]\n",
    "            aided_non_ranked_docs_names=[doc[1] for doc in positive_class_predicted]\n",
    "            '''\n",
    "            Evaluating binary retrieval\n",
    "            '''\n",
    "            precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec=compute_metrics(aided_non_ranked_docs_names\n",
    "                                                                              ,relevant, nonRelevant,k)\n",
    "            aidedNonRanked[topic]=[precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "            '''\n",
    "            Extension towards ranking\n",
    "            '''\n",
    "            if ranking_type=='proba':#sorts according to probabilities\n",
    "                aided_ranked_docs_names=[x for _, x in sorted(zip(y_scores,DTest[topic]), \n",
    "                                                        key=lambda pair: pair[0][1],reverse=True)]\n",
    "            else:#sort according to the score of the docs classified as positive\n",
    "                aided_ranked_docs_names=[doc[1] for doc in sorted(positive_class_predicted,\n",
    "                                               key=lambda x:RRF(x[0]),reverse=True)[:k]]\n",
    "            '''\n",
    "            Evaluating Aided IR\n",
    "            '''\n",
    "            precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec=compute_metrics(aided_ranked_docs_names\n",
    "                                                                              ,relevant, nonRelevant,k)\n",
    "            aidedRanked[topic]=[precision,recall,fscoreVal,precision_recall_curve,bpref,avg_prec]\n",
    "\n",
    "        #MEAN AVERAGE PRECISION\n",
    "        \n",
    "        #print([nonAidedIROutput[topic][4] for topic in nonAidedIROutput])\n",
    "        nonAidedIROutput['Mean MAP'] = mean([nonAidedIROutput[topic][5] for topic in nonAidedIROutput])\n",
    "        aidedRanked['Mean MAP']= mean([aidedRanked[topic][5] for topic in aidedRanked])\n",
    "        aidedNonRanked['Mean MAP']= mean([aidedNonRanked[topic][5] for topic in aidedNonRanked])\n",
    "        \n",
    "        #MEAN BPREF\n",
    "        nonAidedIROutput['Mean MBPREF'] = mean([nonAidedIROutput[topic][4] for topic in nonAidedIROutput if topic != 'Mean MAP'])\n",
    "        aidedRanked['Mean MBPREF']= mean([aidedRanked[topic][4] for topic in aidedRanked if topic != 'Mean MAP'])\n",
    "        aidedNonRanked['Mean MBPREF']= mean([aidedNonRanked[topic][4] for topic in aidedNonRanked if topic != 'Mean MAP'])\n",
    "        classifier_metrics['Mean MAP']=mean(avg_prec_scores)\n",
    "        \n",
    "        return classifier_metrics, nonAidedIROutput,aidedRanked,aidedNonRanked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from scipy.stats import loguniform\n",
    "\n",
    "def hyper_parameter_search(classifier_type,trainX,RTrain):\n",
    "    hyper_params=dict()\n",
    "    space = dict()\n",
    "    space['solver'] = ['lbfgs']\n",
    "    space['penalty'] = ['none', 'l2']\n",
    "    space['C'] = loguniform(1e-5, 100)\n",
    "    model = LogisticRegression(multi_class='multinomial')\n",
    "    #perform topic-conditional hyper-parameter search\n",
    "    for topic in trainX:\n",
    "        if topic != 'R175':\n",
    "            if classifier_type=='logistic':\n",
    "                search = RandomizedSearchCV(model, space, n_iter=100, scoring='accuracy', n_jobs=-1, random_state=1)\n",
    "                # execute search\n",
    "                result = search.fit(trainX[topic], RTrain[topic])\n",
    "                # summarize result\n",
    "                print('Best Score: %s' % result.best_score_)\n",
    "                print('Best Hyperparameters: %s' % result.best_params_)\n",
    "                hyper_params[topic]=result.best_params_\n",
    "    return hyper_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
